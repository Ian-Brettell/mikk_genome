---
title: "MIKK Panel genome analysis workbook"
output: html_notebook
editor_options: 
  chunk_output_type: inline
---

# Plan

*20200203*

From meeting with Tom Fitzgerald on 26 November 2019:

• Introgression:
  - Create giant population VCF - choose the datasets. Just a case of merging some VCFs.
    - Get some Indonesian medaka
• LD decay:
  - LD plots - per chromosome.
  - Heatmap per chromosome?
• Fst plot  

# Setup

## Create directory structure and clone repo

Working directory here: `/hps/research1/birney/users/ian/mikk_paper`

```{bash}
# move to working directory
homehps
cd mikk_paper
# clone git repository
git clone https://github.com/Ian-Brettell/mikk_genome.git
# create directory for VCFs
mkdir vcfs
```

## Pull across MIKK Panel VCF

```{r}
cp /nfs/research1/birney/projects/medaka/inbred_panel/medaka-alignments-release-94/vcf/medaka_inbred_panel_ensembl_new_reference_release_94.vcf* vcfs
```

## Key file for cram ID to line ID

`mikk_genome/data/20200206_cram_id_to_line_id.txt`

## Remove duplicates and non-panel lines

```{bash}
# Find duplicates
ssh ebi
homehps
cd mikk_paper/mikk_genome/
cat data/20200206_cram_id_to_line_id.txt | cut -f2 | cut -f1 -d"_" | sort | uniq -d
```

Note the following duplicates:

-106
-11
-117
-131
-132
-134
-135
-138
-14
-140
-141
-15
-23
-32
-39
-4
-40
-49
-59
-69
-7
-71
-72
-80
-84

Only take _1 sibling from pair, unless what is ecluded is the only survivor based on `mikk_behaviour/data/panel_1/20200109_panel_lines.txt`.

*Query* whether we keep the lines that may have died out? Ask Felix.

## Key file for no sibs

`mikk_genome/data/20200206_cram2line_key_no-sibs.txt`

Excluded IDs: `mikk_genome/data/20200206_excluded_lines.txt`

*20200225*

Full list of MIKK lines from Felix here: `mikk_genome/data/20200210_panel_lines_full.txt`

```{bash}
cat ~/Documents/Repositories/mikk_genome/data/20200210_panel_lines_full.txt cut -f1 -d"-" | sort | uniq -d
```

- 106
- 11
- 117
- 131
- 132
- 135
- 14
- 140
- 23
- 39
- 4
- 40
- 59
- 69
- 72
- 80

List with no sibling lines here: `mikk_genome/data/20200227_panel_lines_no-sibs.txt`. 64 lines total.

Excluded IDs here: `mikk_genome/data/20200227_panel_lines_excluded.txt`. 16 lines total.

Replace all dashes with underscores to match cram2line key file
```{bash}
sed 's/-/_/g' data/20200227_panel_lines_no-sibs.txt > data/20200227_panel_lines_no-sibs_us.txt  
```

Extract the lines to keep from the key file.
```{bash}
awk  'FNR==NR {f1[$0]; next} $2 in f1' data/20200227_panel_lines_no-sibs_us.txt data/20200206_cram_id_to_line_id.txt > data/20200227_cram2line_no-sibs.txt
```

Has 66 lines instead of 63 (because we're missing 130-2), so there must be duplicates. Find out which ones:

```{bash}
cat data/20200227_cram2line_no-sibs.txt | cut -f2 | cut -f1 -d"_" | sort | uniq -d
```

32
71
84

Manually removed (`data/20200227_duplicates_excluded.txt`):

• 24271_7#5	32_2
• 24271_8#4	71_1
• 24259_1#1	84_2

Final version: `data/20200227_cram2line_no-sibs.txt`

Final version, cram IDs only:

# Create filtered VCF

## 
```{bash}

```





