---
title: "Medaka working notebook"
output:
  html_notebook:
      df_print: paged
      number_sections: true
      toc: true
      toc_float: true
  html_document:
    df_print: paged
    number_sections: true
    toc: true
    toc_float: true
editor_options: 
  chunk_output_type: inline
---

*5 September 2018*

# LD analysis

*Tutorial on ABBA BABA here: <http://evomics.org/learning/population-and-speciation-genomics/2018-population-and-speciation-genomics/abba-baba-statistics/>.

## Cluster logins
```{bash, eval = F}
# main cluster
ssh ebi

# yoda
ssh yoda
```


## Relevant paths
```{bash, eval = F}
# main cluster
/nfs/research1/birney
## my directory
/nfs/research1/birney/users/brettell

## medaka fish panel vcf
/nfs/research1/birney/projects/medaka/inbred_panel/WGS_Sanger_x10s_20170407/vcf/medaka_inbred_panel_joint_calls_all_mapped_and_unanchored_panel_trios_mq.vcf.gz

## medaka fish panel split by chromosome
/nfs/research1/birney/projects/medaka/inbred_panel/WGS_Sanger_x10s_20170407/vcf/split_by_chr

# Yoda
/nfs/leia/research/birney

# Group software directories on both main and yoda
/nfs/software/birney
```

TF: "Generally I use Yoda and leia for heavy lifting involving lots of read / write.
And main cluster for most things (I normally use the main cluster)
Very broadly project spaces should be used when working on projects either involving multiple people or that would want to be shared with other people.

But general analysis and scratch type stuff do in your user directory."

## Tips and tricks

### Runnning and submitting

Don't run jobs on head node. Submit using `bsub`. To connect to a node and run jobs interactively (so you can watch the jobs run - good for testing), enter interactive mode: `bsub -Is bash`.

Check in on running jobs with `bpeek <jobid>`.

### Code for submitting jobs from HC
```{bash, eval = F}
bsub -F "select [mem>$mem] rusage [mem=$mem]" \ -M $mem -o <path> \ -e <path>

# where -o is the path for the log file, and -e is the path for the error file
```


*7 September 2018*

## Notes on ABBA BABA

* ABBA BABA is used as a test for **introgression**, meaning "the transfer of genetic information from one species to another as a result of hybridization between them and repeated backcrossing".

* Start with genotype data from multiple individuals, and infer allele frequencies at each SNP.

* Then compute the *D* statistic

* Following meeting with EB, begin to carry out LD analysis.

## LD analysis

* From *Spivakov et al. (2014)*:

"Computations were performed using VCFtools (Danecek
et al. 2011) with the following options: –ld-window-bp 50000, –max-alleles 2, –min-alleles 2, –min-r2 0.001, –geno 0.8. Haplotype blocks in the 16 medaka founders were called using HaploView (Barrett et al. 2005) using the default parameters for the method described by Gabriel et al. (2002)."

### Copy panel VCF to user directory

To unzip and manipulate
```{bash, eval = F}
# copy to personal directory
cp /nfs/research1/birney/projects/medaka/inbred_panel/WGS_Sanger_x10s_20170407/vcf/ medaka_inbred_panel_joint_calls_all_mapped_and_unanchored_panel_trios_mq.vcf.gz /nfs/research1/birney/users/brettell/

# unzip
gunzip medaka_inbred_panel_joint_calls_all_mapped_and_unanchored_panel_trios_mq.vcf.gz

# how many lines?
wc -l medaka_inbred_panel_joint_calls_all_mapped_and_unanchored_panel_trios_mq.vcf
# 25,489,752

# how many samples?
head -1000 /nfs/research1/birney/users/brettell/medaka_inbred_panel_joint_calls_all_mapped_and_unanchored_panel_trios_mq.vcf | grep "#" | grep -v "##" | cut -f 10- | awk '{print NF}'
# 139

# show them
head -1000 /nfs/research1/birney/users/brettell/medaka_inbred_panel_joint_calls_all_mapped_and_unanchored_panel_trios_mq.vcf | grep "#" | grep -v "##" | cut -f 10-
```

### Run VCFtools

Tools docs here: <http://vcftools.sourceforge.net/man_latest.html>.\

```{bash, eval = F}
/nfs/software/birney/vcftools_0.1.13/bin/vcftools --gzvcf /nfs/research1/birney/projects/medaka/inbred_panel/WGS_Sanger_x10s_20170407/vcf/medaka_inbred_panel_joint_calls_all_mapped_and_unanchored_panel_trios_mq.vcf.gz --geno-r2 --ld-window-bp 50000 --max-alleles 2 --min-alleles 2 --min-r2 0.001 --out /nfs/research1/birney/users/brettell/medaka/DATA/geno-r2_all
#Job <2380325> is submitted to default queue <research-rh7>: took over 8 hours to run.
```

*10 September 2018*

File reconciling cram IDs to line IDs here: <https://github.com/Ian-Brettell/medaka/blob/master/DATA/cram_file_to_line_ids.txt>.

115 different lines.

“KW” lines are wild medaka (7 lines), “iCab” (4 lines) and "Ho5” (2 lines) are classical inbred lines - 13 total.

Note the following sibling pairs:

* 23_1 and _2
* 72_1 and _2
* 14_1 and _2
* 39_1 and _2
* 40_1 and _2
* 59_1 and _2
* 106_1 and _2
* 131_2 and _4
* 7_1 and _2
* 15_1 and _2
* 134_1, _2 and _3
* 141_3 and _4

(Keep the first, removing 13 lines)

Also note the following duplications:

* 84_2 (22013_6#1 and 24259_1#1)
* 32_2 (22013_3#4 and 24271_7#5)
* 71_1 (22013_7#2 and 24271_8#4)
* 141_3 (22218_2#1 and 24259_2#3)

(Keep the first, removing 4 lines)

Leaving **85 lines** for LD analysis.

Manually removed them, saving them here: <https://github.com/Ian-Brettell/medaka/blob/master/DATA/cram_file_to_line_ids_edited.txt>.

In accordance with VCFTools' `--keep` argument here: <https://vcftools.github.io/man_0113.html>, we've kept only the cram IDs using `cut -f1 Documents/Repositories/medaka/DATA/cram_file_to_line_ids_edited.txt > Documents/Repositories/medaka/DATA/cram_file_to_line_ids_cram_only.txt`, saving them here: <https://github.com/Ian-Brettell/medaka/blob/master/DATA/cram_file_to_line_ids_cram_only.txt>.

Also created a file with line_ids only using `cut -f2 Documents/Repositories/medaka/DATA/cram_file_to_line_ids_edited.txt > Documents/Repositories/medaka/DATA/line_ids_only_no_sibs.txt`, saving them here: <https://github.com/Ian-Brettell/medaka/blob/master/DATA/line_ids_only_no_sibs.txt>.

```{bash, eval = F}
bsub /nfs/software/birney/vcftools_0.1.13/bin/vcftools --gzvcf /nfs/research1/birney/projects/medaka/inbred_panel/WGS_Sanger_x10s_20170407/vcf/medaka_inbred_panel_joint_calls_all_mapped_and_unanchored_panel_trios_mq.vcf.gz --geno-r2 --ld-window-bp 50000 --max-alleles 2 --min-alleles 2 --min-r2 0.001 --keep /nfs/research1/birney/users/brettell/medaka/DATA/cram_file_to_line_ids_cram_only.txt --out /nfs/research1/birney/users/brettell/medaka/DATA/geno-r2_all
#Job <2999659> is submitted to default queue <research-rh7>.

```

*13 September 2018*

VCFtools LD job is taking days, so will use Plink to carry out the LD analysis as well.

```{bash, eval = F}
# TEST run VCFtools by chromosome
bsub /nfs/software/birney/vcftools_0.1.13/bin/vcftools --gzvcf /nfs/research1/birney/projects/medaka/inbred_panel/WGS_Sanger_x10s_20170407/vcf/medaka_inbred_panel_joint_calls_all_mapped_and_unanchored_panel_trios_mq.vcf.gz --geno-r2 --ld-window-bp 50000 --max-alleles 2 --min-alleles 2 --min-r2 0.001 --keep /nfs/research1/birney/users/brettell/medaka/DATA/cram_file_to_line_ids_cram_only.txt --chr 1 --stdout
# SUCCESS

# TEST run with just chr 24
for i in `seq 24 24`; do bsub /nfs/software/birney/vcftools_0.1.13/bin/vcftools --gzvcf /nfs/research1/birney/projects/medaka/inbred_panel/WGS_Sanger_x10s_20170407/vcf/medaka_inbred_panel_joint_calls_all_mapped_and_unanchored_panel_trios_mq.vcf.gz --geno-r2 --ld-window-bp 50000 --max-alleles 2 --min-alleles 2 --min-r2 0.001 --keep /nfs/research1/birney/users/brettell/medaka/DATA/cram_file_to_line_ids_cram_only.txt --chr $i --out `echo -e "/nfs/research1/birney/users/brettell/medaka/DATA/geno-r2_chr\b$i"`; done
# Job <3652137> is submitted to default queue <research-rh7>.
# Comes out with a weird symbol where the '\b' is.

# TEST try again with different syntax
for i in `seq 23 23`; do bsub /nfs/software/birney/vcftools_0.1.13/bin/vcftools --gzvcf /nfs/research1/birney/projects/medaka/inbred_panel/WGS_Sanger_x10s_20170407/vcf/medaka_inbred_panel_joint_calls_all_mapped_and_unanchored_panel_trios_mq.vcf.gz --geno-r2 --ld-window-bp 50000 --max-alleles 2 --min-alleles 2 --min-r2 0.001 --keep /nfs/research1/birney/users/brettell/medaka/DATA/cram_file_to_line_ids_cram_only.txt --chr $i --out `echo -e "/nfs/research1/birney/users/brettell/medaka/DATA/geno-r2_chr$i"`; done
# Job <3654118> is submitted to default queue <research-rh7>.
# SUCCESS

# Run for real
for i in `seq 1 24`; do bsub /nfs/software/birney/vcftools_0.1.13/bin/vcftools --gzvcf /nfs/research1/birney/projects/medaka/inbred_panel/WGS_Sanger_x10s_20170407/vcf/medaka_inbred_panel_joint_calls_all_mapped_and_unanchored_panel_trios_mq.vcf.gz --geno-r2 --ld-window-bp 50000 --max-alleles 2 --min-alleles 2 --min-r2 0.001 --keep /nfs/research1/birney/users/brettell/medaka/DATA/cram_file_to_line_ids_cram_only.txt --chr $i --out `echo -e "/nfs/research1/birney/users/brettell/medaka/DATA/geno-r2_chr$i"`; done
#Job <3723582> is submitted to default queue <research-rh7>.
#Job <3723583> is submitted to default queue <research-rh7>.
#Job <3723584> is submitted to default queue <research-rh7>.
#Job <3723585> is submitted to default queue <research-rh7>.
#Job <3723586> is submitted to default queue <research-rh7>.
#Job <3723587> is submitted to default queue <research-rh7>.
#Job <3723588> is submitted to default queue <research-rh7>.
#Job <3723589> is submitted to default queue <research-rh7>.
#Job <3723590> is submitted to default queue <research-rh7>.
#Job <3723591> is submitted to default queue <research-rh7>.
#Job <3723592> is submitted to default queue <research-rh7>.
#Job <3723593> is submitted to default queue <research-rh7>.
#Job <3723594> is submitted to default queue <research-rh7>.
#Job <3723595> is submitted to default queue <research-rh7>.
#Job <3723596> is submitted to default queue <research-rh7>.
#Job <3723597> is submitted to default queue <research-rh7>.
#Job <3723598> is submitted to default queue <research-rh7>.
#Job <3723599> is submitted to default queue <research-rh7>.
#Job <3723600> is submitted to default queue <research-rh7>.
#Job <3723601> is submitted to default queue <research-rh7>.
#Job <3723602> is submitted to default queue <research-rh7>.
#Job <3723603> is submitted to default queue <research-rh7>.
#Job <3723604> is submitted to default queue <research-rh7>.
#Job <3723605> is submitted to default queue <research-rh7>.
## NOTE: moved to
/nfs/research1/birney/users/brettell/medaka/DATA/vcftools/ld
```

### PLINK

#### convert VCF into plink format with VCFtools
```{bash, eval = F}
# all
bsub /nfs/software/birney/vcftools_0.1.13/bin/vcftools --gzvcf /nfs/research1/birney/projects/medaka/inbred_panel/WGS_Sanger_x10s_20170407/vcf/medaka_inbred_panel_joint_calls_all_mapped_and_unanchored_panel_trios_mq.vcf.gz --keep /nfs/research1/birney/users/brettell/medaka/DATA/cram_file_to_line_ids_cram_only.txt --out /nfs/research1/birney/users/brettell/medaka/DATA/plink/panel_nosibs --plink
#Job <5022722> is submitted to default queue <research-rh7>.
# Moved to:
/nfs/research1/birney/users/brettell/medaka/DATA/ped/
# namely:
/nfs/research1/birney/users/brettell/medaka/DATA/ped/panel_nosibs.ped
/nfs/research1/birney/users/brettell/medaka/DATA/ped/panel_nosibs.map

# by chromosome
for i in `seq 1 24`; do bsub /nfs/software/birney/vcftools_0.1.13/bin/vcftools --gzvcf /nfs/research1/birney/projects/medaka/inbred_panel/WGS_Sanger_x10s_20170407/vcf/medaka_inbred_panel_joint_calls_all_mapped_and_unanchored_panel_trios_mq.vcf.gz --keep /nfs/research1/birney/users/brettell/medaka/DATA/cram_file_to_line_ids_cram_only.txt --chr $i --out `echo "/nfs/research1/birney/users/brettell/medaka/DATA/plink/chr$i"` --plink ; done
#Job <4871478> is submitted to default queue <research-rh7>.
#Job <4871479> is submitted to default queue <research-rh7>.
#Job <4871480> is submitted to default queue <research-rh7>.
#Job <4871481> is submitted to default queue <research-rh7>.
#Job <4871482> is submitted to default queue <research-rh7>.
#Job <4871483> is submitted to default queue <research-rh7>.
#Job <4871484> is submitted to default queue <research-rh7>.
#Job <4871485> is submitted to default queue <research-rh7>.
#Job <4871486> is submitted to default queue <research-rh7>.
#Job <4871487> is submitted to default queue <research-rh7>.
#Job <4871488> is submitted to default queue <research-rh7>.
#Job <4871489> is submitted to default queue <research-rh7>.
#Job <4871490> is submitted to default queue <research-rh7>.
#Job <4871491> is submitted to default queue <research-rh7>.
#Job <4871492> is submitted to default queue <research-rh7>.
#Job <4871493> is submitted to default queue <research-rh7>.
#Job <4871494> is submitted to default queue <research-rh7>.
#Job <4871495> is submitted to default queue <research-rh7>.
#Job <4871496> is submitted to default queue <research-rh7>.
#Job <4871497> is submitted to default queue <research-rh7>.
#Job <4871498> is submitted to default queue <research-rh7>.
#Job <4871499> is submitted to default queue <research-rh7>.
#Job <4871500> is submitted to default queue <research-rh7>.
#Job <4871501> is submitted to default queue <research-rh7>.

## NOTE: moved all files to:
/nfs/research1/birney/users/brettell/medaka/DATA/ped/by_chromosome
```
**NOTE**: VCFtools creates a PED encoded with A, T, G, C rather than 0, 1, 2. This may cause problems later, e.g. when using the PED for Haploview.


#### convert VCF into plink format (SNPs only)
```{bash, eval = F}
# TEST
bsub /nfs/software/birney/vcftools_0.1.13/bin/vcftools --gzvcf /nfs/research1/birney/projects/medaka/inbred_panel/WGS_Sanger_x10s_20170407/vcf/medaka_inbred_panel_joint_calls_all_mapped_and_unanchored_panel_trios_mq.vcf.gz --keep /nfs/research1/birney/users/brettell/medaka/DATA/cram_file_to_line_ids_cram_only.txt --chr 20 --remove-indels --out /nfs/research1/birney/users/brettell/medaka/DATA/ped_snps_only/test --plink
#Job <4905299> is submitted to default queue <research-rh7>.
```

#### convert VCF into binary format with PLINK2 (pgen)
```{bash, eval = F}
# Entire VCF
bsub /nfs/software/birney/plink2 --vcf /nfs/research1/birney/projects/medaka/inbred_panel/WGS_Sanger_x10s_20170407/vcf/medaka_inbred_panel_joint_calls_all_mapped_and_unanchored_panel_trios_mq.vcf.gz --out /nfs/research1/birney/users/brettell/medaka/DATA/binary/all
#Job <4909515> is submitted to default queue <research-rh7>.
#Error: Invalid chromosome code '000003F' on line 25468448 of --vcf file.
#(Use --allow-extra-chr to force it to be accepted.)
bsub /nfs/software/birney/plink2 --vcf /nfs/research1/birney/projects/medaka/inbred_panel/WGS_Sanger_x10s_20170407/vcf/medaka_inbred_panel_joint_calls_all_mapped_and_unanchored_panel_trios_mq.vcf.gz --allow-extra-chr --out /nfs/research1/birney/users/brettell/medaka/DATA/binary/all
#Job <4909704> is submitted to default queue <research-rh7>.

# Panel only, no siblings
bsub /nfs/software/birney/plink2 --make-pgen --vcf /nfs/research1/birney/projects/medaka/inbred_panel/WGS_Sanger_x10s_20170407/vcf/medaka_inbred_panel_joint_calls_all_mapped_and_unanchored_panel_trios_mq.vcf.gz --keep /nfs/research1/birney/users/brettell/medaka/DATA/cram_file_to_line_ids_cram_only.txt --out /nfs/research1/birney/users/brettell/medaka/DATA/binary/panel_nosibs
# Job <4909616> is submitted to default queue <research-rh7>.
#Error: Invalid chromosome code '000003F' on line 25468448 of --vcf file.
#(Use --allow-extra-chr to force it to be accepted.)
bsub /nfs/software/birney/plink2 --make-pgen --vcf /nfs/research1/birney/projects/medaka/inbred_panel/WGS_Sanger_x10s_20170407/vcf/medaka_inbred_panel_joint_calls_all_mapped_and_unanchored_panel_trios_mq.vcf.gz --keep /nfs/research1/birney/users/brettell/medaka/DATA/cram_file_to_line_ids_cram_only.txt --allow-extra-chr --out /nfs/research1/birney/users/brettell/medaka/DATA/binary/panel_nosibs
#Job <4909712> is submitted to default queue <research-rh7>.

# Panel only, no siblings, SNPs only
bsub /nfs/software/birney/plink2 --make-pgen --vcf /nfs/research1/birney/projects/medaka/inbred_panel/WGS_Sanger_x10s_20170407/vcf/medaka_inbred_panel_joint_calls_all_mapped_and_unanchored_panel_trios_mq.vcf.gz --keep /nfs/research1/birney/users/brettell/medaka/DATA/cram_file_to_line_ids_cram_only.txt --snps-only --allow-extra-chr --out /nfs/research1/birney/users/brettell/medaka/DATA/binary/panel_nosibs_snps_only
#Job <4909715> is submitted to default queue <research-rh7>.
```

#### convert VCF into binary format with PLINK2 (bed)
```{bash, eval = F}
# Panel only, no siblings, SNPs only
bsub /nfs/software/birney/plink2 --make-bed --vcf /nfs/research1/birney/projects/medaka/inbred_panel/WGS_Sanger_x10s_20170407/vcf/medaka_inbred_panel_joint_calls_all_mapped_and_unanchored_panel_trios_mq.vcf.gz --keep /nfs/research1/birney/users/brettell/medaka/DATA/cram_file_to_line_ids_cram_only.txt --snps-only --allow-extra-chr --out /nfs/research1/birney/users/brettell/medaka/DATA/binary/panel_nosibs_snps_only
# Job <4909744> is submitted to default queue <research-rh7>.
# Note: No phenotype data present.
#--keep: 0 samples remaining.
#Error: No samples remaining after main filters.  (Add --allow-no-samples to permit this.)
```

#### convert VCF into PED using VCFtools (SNPs only)
```{bash, eval = F}
# whole panel
bsub /nfs/software/birney/vcftools_0.1.13/bin/vcftools --gzvcf /nfs/research1/birney/projects/medaka/inbred_panel/WGS_Sanger_x10s_20170407/vcf/medaka_inbred_panel_joint_calls_all_mapped_and_unanchored_panel_trios_mq.vcf.gz --remove-indels --keep /nfs/research1/birney/users/brettell/medaka/DATA/cram_file_to_line_ids_cram_only.txt --plink --out /nfs/research1/birney/users/brettell/medaka/DATA/ped_snps_only/panel_nosibs_snps
#Job <4935844> is submitted to default queue <research-rh7>.
#SUCCESS

# by chromosome
/nfs/software/birney/vcftools_0.1.13/bin/vcftools --gzvcf /nfs/research1/birney/projects/medaka/inbred_panel/WGS_Sanger_x10s_20170407/vcf/medaka_inbred_panel_joint_calls_all_mapped_and_unanchored_panel_trios_mq.vcf.gz --remove-indels --keep /nfs/research1/birney/users/brettell/medaka/DATA/cram_file_to_line_ids_cram_only.txt --chr $i --plink --out /nfs/research1/birney/users/brettell/medaka/DATA/ped_snps_only/panel_snps_only_chr$i

for i in `seq 1 24`; do bsub /nfs/software/birney/vcftools_0.1.13/bin/vcftools --gzvcf /nfs/research1/birney/projects/medaka/inbred_panel/WGS_Sanger_x10s_20170407/vcf/medaka_inbred_panel_joint_calls_all_mapped_and_unanchored_panel_trios_mq.vcf.gz --remove-indels --keep /nfs/research1/birney/users/brettell/medaka/DATA/cram_file_to_line_ids_cram_only.txt --chr $i --plink --out /nfs/research1/birney/users/brettell/medaka/DATA/ped_snps_only/panel_snps_only_chr$i; done
```

#### convert VCF into PED with PLINK2

Using the --recode argument <https://www.cog-genomics.org/plink/1.9/data#recode>, creates both a .ped and .info file: <https://www.cog-genomics.org/plink/1.9/formats#info>.

```{bash, eval = F}
bsub /nfs/software/birney/plink2 --recode HV --vcf /nfs/research1/birney/projects/medaka/inbred_panel/WGS_Sanger_x10s_20170407/vcf/medaka_inbred_panel_joint_calls_all_mapped_and_unanchored_panel_trios_mq.vcf.gz --keep /nfs/research1/birney/users/brettell/medaka/DATA/cram_file_to_line_ids_cram_only.txt --allow-extra-chr --out /nfs/research1/birney/users/brettell/medaka/DATA/ped/haploview_format/panel_nosibs
#Job <5369831> is submitted to default queue <research-rh7>.
#--keep: 0 samples remaining.
# Error: No samples remaining after main filters.  (Add --allow-no-samples to permit this.)

# Issue with the way the sample IDs are encoded? See https://www.cog-genomics.org/plink/2.0/filter#sample. Try with all samples to find out whether the PED is interpreting them weirdly.

# TEST
bsub /nfs/software/birney/plink2 --recode HV --vcf /nfs/research1/birney/projects/medaka/inbred_panel/WGS_Sanger_x10s_20170407/vcf/medaka_inbred_panel_joint_calls_all_mapped_and_unanchored_panel_trios_mq.vcf.gz --allow-extra-chr --chr 20 --out /nfs/research1/birney/users/brettell/medaka/DATA/ped/haploview_format/test
## NOTE: A previous attempt still returned the following error, despite it being restricted to chromosome 20
#Error: Invalid chromosome code '000003F' on line 25468448 of --vcf file.
#(Use --allow-extra-chr to force it to be accepted.)
# Therefore --allow-extra-chr is still required.

#Job <5452788> is submitted to default queue <research-rh7>.
#Error: Only VCF, oxford, bgen-1.1, haps, hapslegend, A-transpose, and ind-major-bed output have been implemented so far.
#NOTE: --recode HV is on plink 1.9, not plink 2!!

# Try the --const-fid flag to "convert the sample IDs to individual IDs"
#https://www.cog-genomics.org/plink/2.0/input#double_id

# TEST
bsub /nfs/software/birney/plink2 --recode HV --vcf /nfs/research1/birney/projects/medaka/inbred_panel/WGS_Sanger_x10s_20170407/vcf/medaka_inbred_panel_joint_calls_all_mapped_and_unanchored_panel_trios_mq.vcf.gz --allow-extra-chr --chr 20 --const-fid --out /nfs/research1/birney/users/brettell/medaka/DATA/ped/haploview_format/test2
#Job <5459530> is submitted to default queue <research-rh7>.
```

*19 September 2018*

##### Get plink 1.9
```{bash, eval = F}
cd /nfs/software/birney/
wget https://www.cog-genomics.org/static/bin/plink180913/plink_linux_x86_64.zip
unzip plink_linux_x86_64.zip

# created the following files:
/nfs/software/birney/plink

```

##### Run again in Plink 1.9
```{bash, eval = F}
bsub /nfs/software/birney/plink --recode HV --vcf /nfs/research1/birney/projects/medaka/inbred_panel/WGS_Sanger_x10s_20170407/vcf/medaka_inbred_panel_joint_calls_all_mapped_and_unanchored_panel_trios_mq.vcf.gz --keep /nfs/research1/birney/users/brettell/medaka/DATA/cram_file_to_line_ids_cram_only.txt --allow-extra-chr --out /nfs/research1/birney/users/brettell/medaka/DATA/ped/haploview_format/panel_nosibs
#Job <5461445> is submitted to default queue <research-rh7>.
#139 people (0 males, 0 females, 139 ambiguous) loaded from .fam.
#Ambiguous sex IDs written to /nfs/research1/birney/users/brettell/medaka/DATA/ped/haploview_format/panel_nosibs.nosex
```

Tool docs indicate that the --keep file requires two columns: <https://www.cog-genomics.org/plink/1.9/filter#indiv>
Edit sample file to include second column.
```{bash, eval = F}
# duplicate column
cd /nfs/research1/birney/users/brettell/medaka/DATA/
awk 'BEGIN { OFS="\t" } { $1=$1 "\t" $1 } 1' cram_file_to_line_ids_cram_only.txt > cram_file_to_line_ids_cram_only_plink_format.txt

#run again
bsub /nfs/software/birney/plink --recode HV --vcf /nfs/research1/birney/projects/medaka/inbred_panel/WGS_Sanger_x10s_20170407/vcf/medaka_inbred_panel_joint_calls_all_mapped_and_unanchored_panel_trios_mq.vcf.gz --keep /nfs/research1/birney/users/brettell/medaka/DATA/cram_file_to_line_ids_cram_only_plink_format.txt --allow-extra-chr --out /nfs/research1/birney/users/brettell/medaka/DATA/ped/haploview_format/panel_nosibs
#Job <5486340> is submitted to default queue <research-rh7>.
#139 people (0 males, 0 females, 139 ambiguous) loaded from .fam.
#Ambiguous sex IDs written to /nfs/research1/birney/users/brettell/medaka/DATA/ped/haploview_format/panel_nosibs.nosex. Error: No people remaining after --keep.
```

So it's still reading the sample IDs in such a way that removes all of them. Try --recode --const-fid from Plink2, and then --recode that PED with Plink 1.9.

```{bash, eval = F}
# recode with Plink2 in PED using --const-fid
bsub /nfs/software/birney/plink2 --recode --vcf /nfs/research1/birney/projects/medaka/inbred_panel/WGS_Sanger_x10s_20170407/vcf/medaka_inbred_panel_joint_calls_all_mapped_and_unanchored_panel_trios_mq.vcf.gz --const-fid --allow-extra-chr --out /nfs/research1/birney/users/brettell/medaka/DATA/ped/plink2/panel_nosibs
#Job <5614059> is submitted to default queue <research-rh7>.
#Error: Only VCF, oxford, bgen-1.1, haps, hapslegend, A-transpose, and ind-major-bed output have been implemented so far.
```

So Plink2 can't output a PED? Ok...

*20 September 2018*

What seems to be going wrong with the Plink1.9 recoding is the # symbol in the sample names in the VCF. It may be splitting up that sample ID into the individual ID (before #) and the family ID (after #).

So we'll rename the samples in the VCF using BCFtools' 'reheader' option: <http://www.htslib.org/doc/bcftools.html#reheader>.

```{bash, eval = F}
# create list of all samples
cut -f1 /nfs/research1/birney/users/brettell/medaka/DATA/cram_file_to_line_ids.txt | tail -n +2 > /nfs/research1/birney/users/brettell/medaka/DATA/cram_ids_panel.txt

# remove trios from VCF
/nfs/software/birney/vcftools_0.1.13/bin/vcftools --gzvcf /nfs/research1/birney/projects/medaka/inbred_panel/WGS_Sanger_x10s_20170407/vcf/medaka_inbred_panel_joint_calls_all_mapped_and_unanchored_panel_trios_mq.vcf.gz --keep /nfs/research1/birney/users/brettell/medaka/DATA/cram_ids_panel.txt --recode --out /nfs/research1/birney/users/brettell/kiyosu_panel
#Job <5652006> is submitted to default queue <research-rh7>.
#SUCCESS! Created:
/nfs/research1/birney/users/brettell/kiyosu_panel.recode.vcf

# Do it again to remove the siblings from the VCF for our LD analysis
bsub /nfs/software/birney/vcftools_0.1.13/bin/vcftools --gzvcf /nfs/research1/birney/projects/medaka/inbred_panel/WGS_Sanger_x10s_20170407/vcf/medaka_inbred_panel_joint_calls_all_mapped_and_unanchored_panel_trios_mq.vcf.gz --keep /nfs/research1/birney/users/brettell/medaka/DATA/cram_file_to_line_ids_cram_only.txt --recode --out /nfs/research1/birney/users/brettell/kiyosu_panel_no_sibs
#Job <5737241> is submitted to default queue <research-rh7>.

# create file with new IDs (whole panel, i.e. with siblings and duplicates) - not necessary now for further steps
tail -n +2  /nfs/research1/birney/users/brettell/medaka/DATA/cram_file_to_line_ids.txt | cut -f 2 > /nfs/research1/birney/users/brettell/medaka/DATA/line_ids.txt

# run bcftools to replace cram IDs with line IDs (panel - 115 individuals)
/nfs/software/birney/bcftools/bcftools reheader --samples /nfs/research1/birney/users/brettell/medaka/DATA/line_ids.txt /nfs/research1/birney/users/brettell/kiyosu_panel.recode.vcf > /nfs/research1/birney/users/brettell/kiyosu_panel_line_ids.vcf

# run bcftools to replace cram IDs with line IDs (panel, no siblings - 85 individuals)
/nfs/software/birney/bcftools/bcftools reheader --samples /nfs/research1/birney/users/brettell/medaka/DATA/line_ids_only_no_sibs.txt /nfs/research1/birney/users/brettell/kiyosu_panel_no_sibs.recode.vcf > /nfs/research1/birney/users/brettell/kiyosu_panel_no_sibs_line_ids.vcf

# now reformat nosibs panel (85 individuals) for Haploview using Plink1.9
bsub -M 10000 /nfs/software/birney/plink --recode HV --vcf /nfs/research1/birney/users/brettell/kiyosu_panel_no_sibs_line_ids.vcf --double-id --chr 1-24 --allow-extra-chr --out /nfs/research1/birney/users/brettell/medaka/DATA/ped/haploview_format/panel_nosibs
#include --double-id to avoid it throwing up this error:
#Error: Multiple instances of '_' in sample ID.
#If you do not want '_' to be treated as a FID/IID delimiter, use --double-id or --const-fid to choose a different method of converting VCF sample IDs to PLINK IDs, or --id-delim to change the FID/IID delimiter.
#Job <5780841> is submitted to default queue <research-rh7>.
#SUCCESS... but output with ATGC rather than 012!
```

Try with Haploview
```{bash, eval = F}
java -Xmx20G -jar /nfs/software/birney/Haploview.jar -nogui -pedfile /nfs/research1/birney/users/brettell/medaka/DATA/ped/haploview_format/panel_nosibs.chr-24.ped -memory 10000 -out /nfs/research1/birney/users/brettell/medaka/DATA/haploview/test -blockoutput GAB -dprime -png -chromosome 20 -startpos 1 -endpos 49999
#Pedigree file input error: invalid genotype on line 1
```

So we will need to try to recode the VCF it into a 012 PED using Plink1.9, and then into Haploview format from there.
```{bash, eval = F}
bsub -M 10000 /nfs/software/birney/plink --recode --vcf /nfs/research1/birney/users/brettell/kiyosu_panel_no_sibs_line_ids.vcf --double-id --chr 1-24 --allow-extra-chr --out /nfs/research1/birney/users/brettell/medaka/DATA/ped/panel_nosibs
#Job <5797388> is submitted to default queue <research-rh7>.
#SUCCESS... but still ATGC!!

# obtain SNPs only PED
bsub -M 10000 /nfs/software/birney/plink --recode --vcf /nfs/research1/birney/users/brettell/kiyosu_panel_no_sibs_line_ids.vcf --double-id --chr 1-24 --allow-extra-chr --snps-only --out /nfs/research1/birney/users/brettell/medaka/DATA/ped/panel_nosibs_snpsonly
#Job <5805358> is submitted to default queue <research-rh7>.
# SUCCESS

# recode PED to Haploview format PED
bsub -M 10000 /nfs/software/birney/plink --recode HV --file /nfs/research1/birney/users/brettell/medaka/DATA/ped/panel_nosibs_snpsonly --double-id --chr 1-24 --allow-extra-chr --out /nfs/research1/birney/users/brettell/medaka/DATA/ped/haploview_format/snps_only/panel_nosibs_snps
#Job <5856465> is submitted to default queue <research-rh7>.
#SUCCESS, but still coded as AGCT. Haploview still throws up the error 'Pedigree file input error: invalid genotype on line 1'.

# So let's recode the VCF into a PED with 1234 format, then recode that into Haploview format.

# recode SNPs only VCF into 1234 format
bsub -M 10000 /nfs/software/birney/plink --allele1234 --vcf /nfs/research1/birney/users/brettell/kiyosu_panel_no_sibs_line_ids.vcf --double-id --chr 1-24 --allow-extra-chr --snps-only --make-bed --out /nfs/research1/birney/users/brettell/medaka/DATA/ped/panel_nosibs_snpsonly_1234
#NOTE: requires the --make-bed flag because --allele1234 alone gives: "Error: Basic file conversions do not support regular filtering operations. Rerun your command with --make-bed."
#Job <6161817> is submitted to default queue <research-rh7>.
#SUCCESS

# recode PED into Haploview format
bsub -M 10000 /nfs/software/birney/plink --recode HV --bfile /nfs/research1/birney/users/brettell/medaka/DATA/ped/panel_nosibs_snpsonly_1234 --double-id --chr 1-24 --allow-extra-chr --out /nfs/research1/birney/users/brettell/medaka/DATA/ped/haploview_format/snps_only/1234_format/panel_nosibs_snps_1234
#Job <6169246> is submitted to default queue <research-rh7>.

```

Try running Haploview on the local
```{bash, eval = F}
# download PED and INFO files
scp brettell@ebi:/nfs/research1/birney/users/brettell/medaka/DATA/ped/haploview_format/snps_only/1234_format/panel_nosibs_snps_1234.chr-24* /Users/brettell/Documents/Repositories/medaka/WORKING

scp brettell@ebi:/nfs/research1/birney/users/brettell/medaka/DATA/ped/panel_nosibs_snpsonly.map /Users/brettell/Documents/Repositories/medaka/WORKING

scp brettell@ebi:/nfs/research1/birney/users/brettell/medaka/DATA/ped/panel_nosibs_snpsonly_1234* /Users/brettell/Documents/Repositories/medaka/WORKING
```

Try running Haploview on the cluster
```{bash, eval = F}
bsub -M 10000 java -Xmx20G -jar /nfs/software/birney/Haploview.jar -nogui -pedfile /nfs/research1/birney/users/brettell/medaka/DATA/ped/panel_nosibs_snpsonly_1234.bed -memory 10000 -out /nfs/research1/birney/users/brettell/medaka/DATA/haploview/test -blockoutput GAB -dprime -png -chromosome 20 -startpos 1 -endpos 49999
#Job <6209488> is submitted to default queue <research-rh7>.
# FAILED
```

Create small file to try to work with Haploview
```{bash, eval = F}
# create PED from CHR 20
cut -d" " -f1-10000 /nfs/research1/birney/users/brettell/medaka/DATA/ped/haploview_format/snps_only/1234_format/panel_nosibs_snps_1234.chr-20.ped > /nfs/research1/birney/users/brettell/medaka/DATA/ped/haploview_format/snps_only/1234_format/test.ped

# create INFO from CHR 20
head -9994 /nfs/research1/birney/users/brettell/medaka/DATA/ped/haploview_format/snps_only/1234_format/panel_nosibs_snps_1234.chr-20.info > /nfs/research1/birney/users/brettell/medaka/DATA/ped/haploview_format/snps_only/1234_format/test.info

# send to local
scp brettell@ebi:/nfs/research1/birney/users/brettell/medaka/DATA/ped/haploview_format/snps_only/1234_format/test* /Users/brettell/Documents/Repositories/medaka/WORKING

## NO DISPLAY - "Pedigree file input error: invalid genotype on line 2"

# Noticed that there are asterisks in the file, as well as 0s. This must be causing the error. There are only 44 instances in the test file, around line 26.

# Replacing * with 0 in a text editor appears to resolve the problem - Haploview runs!
```

So we'll copy all the files in that folder to another folder, replace the asterisks with 0, and then try to run Haploview on them on the cluster.

```{bash, eval = F}
# copy files to another folder
cp /nfs/research1/birney/users/brettell/medaka/DATA/ped/haploview_format/snps_only/1234_format/* /nfs/research1/birney/users/brettell/medaka/DATA/ped/haploview_format/snps_only/1234_format/cleaned/

# replace asterisks
for i in `ls /nfs/research1/birney/users/brettell/medaka/DATA/ped/haploview_format/snps_only/1234_format/cleaned/`; do sed -i 's/\*/0/g' $i; done
```

Then test with 20000 SNPs
```{bash, eval = F}
# create PED from CHR 20
cut -d" " -f1-20000 /nfs/research1/birney/users/brettell/medaka/DATA/ped/haploview_format/snps_only/1234_format/cleaned/panel_nosibs_snps_1234.chr-20.ped > /nfs/research1/birney/users/brettell/medaka/DATA/ped/haploview_format/snps_only/1234_format/cleaned/test2.ped

# create INFO from CHR 20
head -19994 /nfs/research1/birney/users/brettell/medaka/DATA/ped/haploview_format/snps_only/1234_format/cleaned/panel_nosibs_snps_1234.chr-20.info > /nfs/research1/birney/users/brettell/medaka/DATA/ped/haploview_format/snps_only/1234_format/cleaned/test2.info

# send to local
scp brettell@ebi:/nfs/research1/birney/users/brettell/medaka/DATA/ped/haploview_format/snps_only/1234_format/cleaned/test2* /Users/brettell/Documents/Repositories/medaka/WORKING

#Worked, but froze when trying to save graph.
```

Test out Haploview on the cluster

```{bash, eval = F}
# find out which chr has the fewest SNPS
for i in `ls /nfs/research1/birney/users/brettell/medaka/DATA/ped/haploview_format/snps_only/1234_format/cleaned/panel_nosibs_snps_1234.*info`; do wc -l $i; done
# range from 633837 (chr20) to 1282205 (chr1)


bsub -F "select [mem>$mem] rusage [mem=$mem]" \ -M $mem -o <path> \ -e <path>


bsub -M 20000 -o /nfs/research1/birney/users/brettell/hptest -e /nfs/research1/birney/users/brettell/hptest_error java -Xmx20G -jar /nfs/software/birney/Haploview.jar -nogui -pedfile /nfs/research1/birney/users/brettell/medaka/DATA/ped/haploview_format/snps_only/1234_format/cleaned/panel_nosibs_snps_1234.chr-20.ped -info /nfs/research1/birney/users/brettell/medaka/DATA/ped/haploview_format/snps_only/1234_format/cleaned/panel_nosibs_snps_1234.chr-20.info -memory 20000 -out /nfs/research1/birney/users/brettell/medaka/DATA/haploview/test -blockoutput GAB -dprime -compressedpng
#Job <6269385> is submitted to default queue <research-rh7>.
# Output says user killed the job?
```

#### Run LD analysis on Plink

```{bash, eval = F}
bsub /nfs/software/birney/plink --r2 bin --file /nfs/research1/birney/users/brettell/medaka/DATA/ped/by_chromosome/chr20 --out /nfs/research1/birney/users/brettell/medaka/DATA/plink/ld/test
# NOTE: --r2 cannot be used with --blocks, and --ld-window-kb cannot be used if the output is a matrix
#Job <6271261> is submitted to default queue <research-rh7>.
#"Exited with exit code 5."?
#In /nfs/research1/birney/users/brettell/medaka/DATA/plink/ld/test.log:
#"Error: Gigantic (over 400k loci) --r/--r2 unfiltered, non-distributed computation.  Rerun with the 'yes-really' modifier if you are SURE you have enough hard drive space and want to do this."
```


##### Create test file

In the meantime, I'll test out the LDheatmap package in R on part of chromosome 20: <https://cran.r-project.org/web/packages/LDheatmap/index.html>.

```{bash, eval = F}
# Take the first 1M lines from the chr20 LD file
head -1000000 /nfs/research1/birney/users/brettell/medaka/DATA/vcftools/ld/geno-r2_chr20.geno.ld > /nfs/research1/birney
/users/brettell/medaka/DATA/vcftools/ld/test.txt

# Send to local
scp brettell@ebi-login-001:/nfs/research1/birney/users/brettell/medaka/DATA/vcftools/ld/test.txt /Users/brettell/Documents/Repositories/medaka/WORKING

```

##### Run on file

```{r}
# install and load pacakage
source("https://bioconductor.org/biocLite.R")
biocLite("snpStats")
install.packages("LDheatmap")
library(LDheatmap)

# import data
test_data <- read.table("/Users/brettell/Documents/Repositories/medaka/WORKING/test.txt", header = T)

# examples
data("CEUData")
LDheatmap(matrix(runif(100,0,1),nrow = 10))
data(GIMAP5)

View(matrix(runif(100,0,1),nrow = 10))
```

##### Run LD analysis in Plink to get matrix

*17 September 2018*

## LD analysis using Haploview

### Haploview tutorial

In preparation for using the software to visualise the LD output, I completed the tutorial available here: <https://www.broadinstitute.org/haploview/tutorial>.

Relevant paths
```{bash, eval = F}
# software here
/Users/brettell/Documents/Software/Haploview.jar

# tutorial samples here
/Users/brettell/Documents/Software/Haploview_tutorial/Samples/

# open software
java -jar Documents/Software/Haploview.jar
```

### Get lengths of each chromosome from VCF
```{bash, eval = F}
# extract chromosome lengths
for i in `head -1000 medaka_inbred_panel_joint_calls_all_mapped_and_unanchored_panel_trios_mq.vcf | grep "##contig" | head -24`; do j=`echo $i | awk -F= '{print$3}' | awk -F, '{print$1}'`; k=`echo $i | awk -F= '{print $4}' | tr -d ">"`; echo $j $k; done
# for each i, j will print the chromosome, k will print its length

# 50kb blocks (0 - 49,999)
for i in `seq 0 50000 500000`; do echo "-startpos $i" "-endpos `expr $i + 49999`"; done

# 50kb blocks (50,000 - )
```

* 1,length=37713152
* 2,length=25379070
* 3,length=38248663
* 4,length=32868862
* 5,length=33205099
* 6,length=32246747
* 7,length=34573382
* 8,length=26239357
* 9,length=33399407
* 10,length=31218526
* 11,length=28210532
* 12,length=30543476
* 13,length=33825776
* 14,length=30598983
* 15,length=30476034
* 16,length=32958677
* 17,length=31792230
* 18,length=30918796
* 19,length=25472880
* 20,length=25942153
* 21,length=31148813
* 22,length=28976614
* 23,length=24400806
* 24,length=23682337


### Haploview analysis

Using the SNPs only PED created by VCFtools
```{bash, eval = F}
# download Haploview to main cluster
cd /nfs/software/birney
wget https://www.broadinstitute.org/ftp/pub/mpg/haploview/Haploview.jar

# TEST


bsub -M 20000 java -Xmx20G -jar /nfs/software/birney/Haploview.jar -nogui -pedfile /nfs/research1/birney/users/brettell/medaka/DATA/ped_snps_only/panel_nosibs_snps.ped -memory 20000 -out /nfs/research1/birney/users/brettell/medaka/DATA/haploview/test -blockoutput GAB -dprime -png -chromosome 20 -startpos 1 -endpos 49999
#Job <4997920> is submitted to default queue <research-rh7>.
#Fatal Error: Exception in thread "main" java.lang.OutOfMemoryError: Java heap space
```

***

*18 September 2018*

## IBD analysis

Using Plink 1.7
```{bash, eval = F}
bsub -M 20000 /nfs/software/birney/plink-1.07-x86_64/plink --noweb --file /nfs/research1/birney/users/brettell/medaka/DATA/ped/panel_nosibs --Z-genome --out /nfs/research1/birney/users/brettell/medaka/DATA/plink/ibd/panel_nosibs
#Job <5599987> is submitted to default queue <research-rh7>.
```

*24 September 2018*

## LD Decay

The SNPs only PED is here: `/nfs/research1/birney/users/brettell/medaka/DATA/ped_snps_only/panel_nosibs_snps.ped`.

Create new PED from VCF with SNPs only, no missing genotypes
```{bash, eval = F}
bsub -M 5000 /nfs/software/birney/plink --vcf /nfs/research1/birney/users/brettell/kiyosu_panel_no_sibs_line_ids.vcf --double-id --allow-extra-chr --chr-set 24 --chr 1-24 --snps-only --geno 0 --biallelic-only --make-bed --out /nfs/research1/birney/users/brettell/medaka/DATA/ped_snps_only/panel_nosibs_snps_no_missing_genos
#--double-id prevents an error from there being an underscore in the sample names
#--biallelic-only: https://www.cog-genomics.org/plink/1.9/input
#Job <7527298> is submitted to default queue <research-rh7>.

```

How many SNPs total?
```{bash, eval = F}
wc -l /nfs/research1/birney/users/brettell/medaka/DATA/ped_snps_only/panel_nosibs_snps_no_missing_genos.bim
#14081474
```

<!--
Make the PED a BED.
```{bash, eval = F}
#For chromosomes 1 to 24
bsub -M 5000 /nfs/software/birney/plink --file /nfs/research1/birney/users/brettell/medaka/DATA/ped_snps_only/panel_nosibs_snps --make-bed --out /nfs/research1/birney/users/brettell/medaka/DATA/ped_snps_only/panel_nosibs_snps_chr1-24 --allow-extra-chr --chr 1-24
#Job <7015249> is submitted to default queue <research-rh7>.
#SUCCESS

#For chromosomes 2 to 24
bsub -M 5000 /nfs/software/birney/plink --file /nfs/research1/birney/users/brettell/medaka/DATA/ped_snps_only/panel_nosibs_snps --make-bed --out /nfs/research1/birney/users/brettell/medaka/DATA/ped_snps_only/panel_nosibs_snps_chr2-24 --allow-extra-chr --chr 2-24
#Job <7015258> is submitted to default queue <research-rh7>.
#SUCCESS
wc -l panel_nosibs_snps_chr2-24.bim
#18267264
```


Following this guide to LD decay here: <https://www.biostars.org/p/300381/>.

Download and unzip mapthin software
```{bash, eval = F}
cd /nfs/software/birney/
wget https://www.staff.ncl.ac.uk/richard.howey/mapthin/mapthin-v1.11-linux-x86_64.zip
unzip mapthin-v1.11-linux-x86_64.zip
chmod +x /nfs/software/birney/mapthin-v1.11-linux-x86_64/mapthin
```
Software here: `/nfs/software/birney/mapthin-v1.11-linux-x86_64/mapthin`

Run Mapthin over the to get 20 SNPs per 1MB
```{bash,eval = F}
/nfs/software/birney/mapthin-v1.11-linux-x86_64/mapthin -b 20 /nfs/research1/birney/users/brettell/medaka/DATA/ped_snps_only/panel_nosibs_snps_chr2-24.bim /nfs/research1/birney/users/brettell/medaka/DATA/ped_snps_only/panel_nosibs_snps_chr2-24_thin.bim
#Total number of SNPs in original file: 18267264
#Number of SNPs in thinned file: 13935 (0.076284%)

#Mean base pair position (in file) between SNPs: 49921 bpp
#St. dev. of base pair position (in file) between SNPs: 542.233 bpp
#Range of base pair position (in file) between SNPs: (34387, 65680)
```

Run plink on thinned file
```{bash, eval = F}
bsub /nfs/software/birney/plink --bfile /nfs/research1/birney/users/brettell/medaka/DATA/ped_snps_only/panel_nosibs_snps_chr2-24 --bim /nfs/research1/birney/users/brettell/medaka/DATA/ped_snps_only/panel_nosibs_snps_chr2-24_thin.bim --r2 --ld-window-r2 0 --ld-window 999999 --ld-window-kb 35000 --out /nfs/research1/birney/users/brettell/medaka/DATA/plink/ld/decay/panel_nosibs_snps_chr2-24_thin
# --ld-window-kb 35000 because that is the longest length of any chromosome
#Error: Invalid .bed file size (expected 306573 bytes).
# We have the pruned bim, but not the bed. Plink doesn't like this. Let's try filtering in Plink.
```

Filter using Plink for 0.03% of SNPs
```{bash, eval = F}
bsub -M 5000 /nfs/software/birney/plink --bfile /nfs/research1/birney/users/brettell/medaka/DATA/ped_snps_only/panel_nosibs_snps_chr2-24 --make-bed --thin 0.03 --out /nfs/research1/birney/users/brettell/medaka/DATA/ped_snps_only/panel_nosibs_snps_chr2-24_thin
#Job <7313414> is submitted to default queue <research-rh7>.
#SUCCESS

wc -l /nfs/research1/birney/users/brettell/medaka/DATA/ped_snps_only/panel_nosibs_snps_chr2-24_thin.bim
#547714
```

-->

But running R2 over the entire file will run pairwise comparisions between even the chromosomes. We need to divide out the BEDs by chromosome first, then thin.
```{bash, eval = F}
bsub -Is bash
for i in `seq 1 24`; do /nfs/software/birney/plink --bfile /nfs/research1/birney/users/brettell/medaka/DATA/ped_snps_only/panel_nosibs_snps_no_missing_genos --make-bed --chr $i --chr-set 24 --out /nfs/research1/birney/users/brettell/medaka/DATA/ped_snps_only/by_chromosome/panel_nosibs_snps_chr$i; done
#NOTE: --geno gets rid of variants with a missing call rate above 0. This ensures that 'missing' is not counted as an allele in the LD analysis, or otherwise does not distort it. THIS REMOVED ROUGHLY 1/3 OF VARIANTS. But when you look at the BIM file, there are still plenty of SNPs with 0 as one of the alleles??
#NOTE: --chr-set sets the number of autosomes
#NOTE: had to run it on the cluster in interactive mode, because otherwise it was throwing up the following error: "Unable to read output data from the stdout buffer file </ebi/lsf/ebi-spool/02/1537814055.7490938.out>: your job was probably aborted prematurely."
```

Thin BEDs using Plink
```{bash, eval = F}
for i in `seq 1 24`; do /nfs/software/birney/plink --bfile /nfs/research1/birney/users/brettell/medaka/DATA/ped_snps_only/by_chromosome/panel_nosibs_snps_chr$i --make-bed --thin 0.001 --out /nfs/research1/birney/users/brettell/medaka/DATA/ped_snps_only/by_chromosome/thinned/panel_nosibs_snps_chr$i\_thin; done
```

Run R2 analysis over the thinned BEDs
```{bash, eval = F}
for i in `seq 1 24`; do /nfs/software/birney/plink --bfile /nfs/research1/birney/users/brettell/medaka/DATA/ped_snps_only/by_chromosome/thinned/panel_nosibs_snps_chr$i\_thin --r2 --ld-window-r2 0 --ld-window 999999 --ld-window-kb 35000 --chr-set 24 --out /nfs/research1/birney/users/brettell/medaka/DATA/plink/ld/decay/panel_nosibs_snps_chr$i\_thin; done
#NOTE: --chr-set sets the number of autosomes
```

Pull out distance and R2
```{bash, eval = F}
for i in `seq 1 24`; do cat /nfs/research1/birney/users/brettell/medaka/DATA/plink/ld/decay/panel_nosibs_snps_chr$i\_thin.ld | sed 1,1d | awk -F " " 'function abs(v) {return v < 0 ? -v : v}BEGIN{OFS="\t"}{print abs($5-$2),$7}' | sort -k1,1n > /nfs/research1/birney/users/brettell/medaka/DATA/plink/ld/decay/ld_by_dist/chr$i.txt; done
```

Copy to local
```{bash, eval = F}
scp brettell@ebi:/nfs/research1/birney/users/brettell/medaka/DATA/plink/ld/decay/ld_by_dist/* /Users/brettell/Documents/Repositories/medaka/WORKING/ld_by_dist/
```

### On R

#### Whole chromosome

Import files into R
```{r}
temp <- list.files(path = "~/Documents/Repositories/medaka/WORKING/ld_by_dist/", full.names = T)
ld_by_dist_1 <- lapply(temp, read.table)
temp <- list.files("~/Documents/Repositories/medaka/WORKING/ld_by_dist/")
names(ld_by_dist_1) <- gsub(".txt", "", list.files("~/Documents/Repositories/medaka/WORKING/ld_by_dist/"))

rm(temp)
```

Plot
```{r}

lapply(names(ld_by_dist_1), function(x){
  y <- ld_by_dist_1[[x]]
  ggplot(y, aes(x = V1, y = V2)) +
    geom_point() +
    geom_smooth() +
    ggtitle(x) +
    labs(x = "Distance (bp)", y = "Linkage Disequilibrium (R squared)")
})
```

Plot again for SNPs up to 1Mb
```{r}

lapply(names(ld_by_dist_1), function(x){
  y <- ld_by_dist_1[[x]]
  ggplot(y, aes(x = V1, y = V2)) +
    geom_point() +
    geom_smooth() +
    ggtitle(x) +
    labs(x = "Distance (bp)", y = "Linkage Disequilibrium (R squared)") +
    xlim(1, 1000000)
})
```

*25 September 2018*

### Run again on variants with frequency of > 0.03

Make new BED excluding those variants
```{bash, eval = F}
bsub /nfs/software/birney/plink --bfile /nfs/research1/birney/users/brettell/medaka/DATA/ped_snps_only/panel_nosibs_snps_no_missing_genos --maf 0.03 --make-bed --chr-set 24 --out /nfs/research1/birney/users/brettell/medaka/DATA/ped_snps_only/panel_nosibs_snps_no_missing_genos_freq_above_0.03
#Job <7933009> is submitted to default queue <research-rh7>.
#9629736 variants removed due to minor allele threshold(s) (--maf/--max-maf/--mac/--max-mac).
#4451738 variants and 85 samples pass filters and QC.


```
**4,451,738 variants** in this BED.

Divide by chromosome
```{bash, eval = F}
bsub -Is bash
for i in `seq 1 24`; do /nfs/software/birney/plink --bfile /nfs/research1/birney/users/brettell/medaka/DATA/ped_snps_only/panel_nosibs_snps_no_missing_genos_freq_above_0.03 --make-bed --chr $i --chr-set 24 --out /nfs/research1/birney/users/brettell/medaka/DATA/ped_snps_only/by_chromosome/maf_threshold/panel_nosibs_snps_maf0.03_chr$i; done
#DONE
```

**NOTE**: Tested R2 with chr24 and it exceeded the given limit of 5000 MB, so we will thin

Thin BEDs using Plink
```{bash, eval = F}
for i in `seq 1 24`; do /nfs/software/birney/plink --bfile /nfs/research1/birney/users/brettell/medaka/DATA/ped_snps_only/by_chromosome/maf_threshold/panel_nosibs_snps_maf0.03_chr$i --make-bed --thin 0.025 --chr-set 24 --out /nfs/research1/birney/users/brettell/medaka/DATA/ped_snps_only/by_chromosome/thinned/maf_threshold/chr$i; done
#DONE
```

Run R2 analysis over the BEDs
```{bash, eval = F}
for i in `seq 1 24`; do bsub -M 5000 /nfs/software/birney/plink --bfile /nfs/research1/birney/users/brettell/medaka/DATA/ped_snps_only/by_chromosome/thinned/maf_threshold/chr$i --r2 --ld-window-r2 0 --ld-window 999999 --ld-window-kb 1000 --chr-set 24 --out /nfs/research1/birney/users/brettell/medaka/DATA/plink/ld/decay/maf_threshold/chr$i; done
#DONE

```

Pull out distance and R2
```{bash, eval = F}
for i in `seq 1 24`; do cat /nfs/research1/birney/users/brettell/medaka/DATA/plink/ld/decay/maf_threshold/chr$i.ld | sed 1,1d | awk -F " " 'function abs(v) {return v < 0 ? -v : v}BEGIN{OFS="\t"}{print abs($5-$2),$7}' | sort -k1,1n > /nfs/research1/birney/users/brettell/medaka/DATA/plink/ld/decay/ld_by_dist/maf_threshold/chr$i.txt; done

```

Copy to local
```{bash, eval = F}
scp brettell@ebi:/nfs/research1/birney/users/brettell/medaka/DATA/plink/ld/decay/ld_by_dist/maf_threshold/* /Users/brettell/Documents/Repositories/medaka/WORKING/ld_by_dist/maf_threshold/
```

Import files into R
```{r}
#import into list
temp <- list.files(path = "~/Documents/Repositories/medaka/WORKING/ld_by_dist/maf_threshold/", full.names = T)
ld_by_dist_maf <- lapply(temp, read.table)
temp <- list.files("~/Documents/Repositories/medaka/WORKING/ld_by_dist/maf_threshold/")

# create list names
chr_names <- gsub(".txt", "", list.files("~/Documents/Repositories/medaka/WORKING/ld_by_dist/maf_threshold/"))
names(ld_by_dist_maf) <- gsub("chr", "", chr_names)

# reorder list
ld_by_dist_maf_ord <- ld_by_dist_maf[order(as.numeric(names(ld_by_dist_maf)))]

# tidy up
rm(temp)
rm(chr_names)
```

Plot
```{r}
lapply(names(ld_by_dist_maf_ord), function(x){
  y <- ld_by_dist_maf[[x]]
  ggplot(y, aes(x = V1, y = V2)) +
    geom_point(size = 0.1) +
    geom_smooth(colour = "red") +
    ggtitle(paste("chr ", x, " - MAF > 0.03", sep = "")) +
    labs(x = "Distance (bp)", y = "Linkage Disequilibrium (R squared)")
})
```

Plot zoomed in to first 25Kb
```{r}
lapply(names(ld_by_dist_maf_ord), function(x){
  y <- ld_by_dist_maf[[x]]
  ggplot(y, aes(x = V1, y = V2)) +
    geom_point(size = 0.1) +
    geom_smooth(colour = "red") +
    ggtitle(paste("chr ", x, " - MAF > 0.03", sep = "")) +
    xlim(0, 25000) +
    labs(x = "Distance (bp)", y = "Linkage Disequilibrium (R squared)")
})
```


## Get haplotype blocks

```{bash, eval = F}
for i in `seq 1 24`; do bsub /nfs/software/birney/plink --bfile /nfs/research1/birney/users/brettell/medaka/DATA/ped_snps_only/by_chromosome/panel_nosibs_snps_chr$i --blocks no-pheno-req --out /nfs/research1/birney/users/brettell/medaka/DATA/plink/blocks/chr$i; done

```

## Plot MAF BEDs in Haploview

Convert into HV format
```{bash, eval = F}
/nfs/software/birney/plink --bfile /nfs/research1/birney/users/brettell/medaka/DATA/ped_snps_only/panel_nosibs_snps_no_missing_genos_freq_above_0.03 --recode HV --chr-set 24 --thin 0.025 --allele1234 --out /nfs/research1/birney/users/brettell/medaka/DATA/haploview/maf_thinned/panel
```

Remove asterisks
```{bash, eval = F}
for i in `ls /nfs/research1/birney/users/brettell/medaka/DATA/haploview/maf_thinned/`; do sed -i 's/\*/0/g' $i; done
```

Copy to local
```{bash, eval = F}
scp brettell@ebi:/nfs/research1/birney/users/brettell/medaka/DATA/haploview/maf_thinned/* /Users/brettell/Documents/Repositories/medaka/WORKING/haploview/
```

Create plots (on local)

```{bash, eval = F}
java -Xmx20G -jar Documents/Software/Haploview.jar -pedfile /Users/brettell/Documents/Repositories/medaka/WORKING/haploview/panel.chr-20.ped -memory 10000 -maxDistance 1000 -out /nfs/research1/birney/users/brettell/medaka/DATA/haploview/plots/test -png
```

*28 September 2018*

## Find largely heterozygous variants

### Create VCF with no missing genotypes
```{bash, eval = F}
bsub -M 5000 /nfs/software/birney/plink --vcf /nfs/research1/birney/users/brettell/kiyosu_panel_no_sibs_line_ids.vcf --double-id --allow-extra-chr --chr-set 24 --chr 1-24 --snps-only --geno 0 --biallelic-only --maf 0.03 --recode vcf-iid --out /nfs/research1/birney/users/brettell/kiyosu_panel_no_sibs_line_ids_snps_only_biallelic_only_no_missing_maf0.03
#--vcf-iid causes the within-family IDs to be used as the sample IDs.
#Job <9144946> is submitted to default queue <research-rh7>.
```
**4,451,738** variants

To get counts, try using *vcflib*: <https://github.com/vcflib/vcflib#vcfhetcount>.
Cloned repository here: `/nfs/software/birney/vcflib/`.
```{bash, eval = F}
/nfs/software/birney/vcflib/bin/vcfhethomratio /nfs/research1/birney/users/brettell/kiyosu_panel_no_sibs_line_ids_snps_only_biallelic_only_no_missing_maf0.03.vcf
#DIDN'T WORK
```

Bash script copied and adapted from here: <https://www.biostars.org/p/291147/#291167>.
Used to create file in Plink gene-set format described here: <https://www.cog-genomics.org/plink/1.9/resources#genelist>
```{bash, eval = F}
# bash script to create a file with the SNPs to be removed
/nfs/software/birney/bcftools/bcftools view /nfs/research1/birney/users/brettell/kiyosu_panel_no_sibs_line_ids_snps_only_biallelic_only_no_missing_maf0.03.vcf | \
awk -F"\t" '{line=$0} BEGIN {
         print "CHR\tPOS\tID\tREF\tALT\tAltHetCount\tAltHomCount\tRefHomCount"
    } !/^#/ {
        if (gsub(/,/, ",", $5)==0) {
            print $1"\t"$2"\t"$3"\t"$4"\t"$5"\t" gsub(/0\|1|1\|0|0\/1|1\/0/,"") "\t" gsub(/1\/1|1\|1/,"") "\t" gsub(/0\/0|0\|0/,"")
        } else if (gsub(/,/, ",", $5)==1) {
            print $1"\t"$2"\t"$3"\t"$4"\t"$5"\t" gsub(/1\/0|0\/1|1\|0|0\|1|1\|2|2\|1|1\/2|2\/1/,"")","gsub(/2\/0|0\/2|2\|0|0\|2|1\|2|2\|1|1\/2|2\/1/,"",line) "\t" gsub(/1\/1|1\|1/,"")","gsub(/2\/2|2\|2/,"") "\t" gsub(/0\/0|0\|0/,"")
        }
    }' | cat | awk ' NR>1{
if ($6 >= 68)
print $1"\t"$2"\t"$2"\t"$1":"$2
}' > /nfs/research1/birney/users/brettell/over_0.8_het_vars_gene_list.txt
```
Excludes **36,412** variants.

Run plink to create BED, excluding those variants
```{bash, eval = F}
/nfs/software/birney/plink --vcf /nfs/research1/birney/users/brettell/kiyosu_panel_no_sibs_line_ids_snps_only_biallelic_only_no_missing_maf0.03.vcf --exclude range /nfs/research1/birney/users/brettell/over_0.8_het_vars_gene_list.txt --make-bed --chr-set 24 --double-id --out  /nfs/research1/birney/users/brettell/medaka/DATA/ped_snps_only/panel_no_sibs_line_ids_snps_only_biallelic_only_no_missing_maf0.03_excl_high_het
```
Leaves **4,415,326** variants.

Divide by chromosome
```{bash, eval = F}
bsub -Is bash
for i in `seq 1 24`; do /nfs/software/birney/plink --bfile /nfs/research1/birney/users/brettell/medaka/DATA/ped_snps_only/panel_no_sibs_line_ids_snps_only_biallelic_only_no_missing_maf0.03_excl_high_het --make-bed --chr $i --chr-set 24 --out /nfs/research1/birney/users/brettell/medaka/DATA/ped_snps_only/by_chromosome/maf_threshold/no_high_het/chr$i; done
#DONE
```

**NOTE**: Tested R2 with chr24 and it exceeded the given limit of 5000 MB, so we will thin

Thin BEDs using Plink
```{bash, eval = F}
for i in `seq 1 24`; do /nfs/software/birney/plink --bfile /nfs/research1/birney/users/brettell/medaka/DATA/ped_snps_only/by_chromosome/maf_threshold/no_high_het/chr$i --make-bed --thin 0.025 --chr-set 24 --out /nfs/research1/birney/users/brettell/medaka/DATA/ped_snps_only/by_chromosome/thinned/maf_threshold/no_high_het/chr$i; done
#DONE
```

Run R2 analysis over the BEDs
```{bash, eval = F}
for i in `seq 1 24`; do bsub -M 5000 /nfs/software/birney/plink --bfile /nfs/research1/birney/users/brettell/medaka/DATA/ped_snps_only/by_chromosome/thinned/maf_threshold/no_high_het/chr$i --r2 --ld-window-r2 0 --ld-window 999999 --ld-window-kb 1000 --chr-set 24 --out /nfs/research1/birney/users/brettell/medaka/DATA/plink/ld/decay/maf_threshold/no_high_het/chr$i; done
#DONE

```

Pull out distance and R2
```{bash, eval = F}
for i in `seq 1 24`; do cat /nfs/research1/birney/users/brettell/medaka/DATA/plink/ld/decay/maf_threshold/no_high_het/chr$i.ld | sed 1,1d | awk -F " " 'function abs(v) {return v < 0 ? -v : v}BEGIN{OFS="\t"}{print abs($5-$2),$7}' | sort -k1,1n > /nfs/research1/birney/users/brettell/medaka/DATA/plink/ld/decay/ld_by_dist/maf_threshold/no_high_het/chr$i.txt; done
#DONE
```

Copy to local
```{bash, eval = F}
scp brettell@ebi:/nfs/research1/birney/users/brettell/medaka/DATA/plink/ld/decay/ld_by_dist/maf_threshold/no_high_het/* /Users/brettell/Documents/Repositories/medaka/WORKING/ld_by_dist/maf_threshold/no_high_het
```

Import files into R
```{r}
#import into list
temp <- list.files(path = "~/Documents/Repositories/medaka/WORKING/ld_by_dist/maf_threshold/no_high_het", full.names = T)
ld_by_dist_maf_nohets <- lapply(temp, read.table)

# create list names
chr_names <- gsub(".txt", "", list.files("~/Documents/Repositories/medaka/WORKING/ld_by_dist/maf_threshold/no_high_het"))
names(ld_by_dist_maf_nohets) <- gsub("chr", "", chr_names)

# reorder list
ld_by_dist_maf_nohets <- ld_by_dist_maf_nohets[order(as.numeric(names(ld_by_dist_maf_nohets)))]

# tidy up
rm(temp)
rm(chr_names)
```

Plot (1Mb)
```{r}
lapply(names(ld_by_dist_maf_nohets), function(x){
  y <- ld_by_dist_maf_nohets[[x]]
  ggplot(y, aes(x = V1, y = V2)) +
    geom_point(size = 0.1) +
    geom_smooth(colour = "red") +
    ggtitle(paste("chr ", x, " - MAF > 0.03", sep = "")) +
    labs(x = "Distance (bp)", y = "Linkage Disequilibrium (R squared)")
})
```

Plot (25Kb)
```{r}
lapply(names(ld_by_dist_maf_nohets), function(x){
  y <- ld_by_dist_maf_nohets[[x]]
  ggplot(y, aes(x = V1, y = V2)) +
    geom_point(size = 0.1) +
    geom_smooth(colour = "red") +
    ggtitle(paste("chr ", x, " - MAF > 0.03", sep = "")) +
    labs(x = "Distance (bp)", y = "Linkage Disequilibrium (R squared)") +
    xlim(0, 25000)
})


lapply(names(ld_by_dist_maf_nohets)[20:24], function(x){
  y <- ld_by_dist_maf_nohets[[x]]
  ggplot(y, aes(x = V1, y = V2)) +
    geom_point(size = 0.1) +
    geom_smooth(colour = "red") +
    ggtitle(paste("chr ", x, " - MAF > 0.03", sep = "")) +
    labs(x = "Distance (bp)", y = "Linkage Disequilibrium (R squared)") +
    xlim(0, 25000)
})
```

### Plot in Haploview

Convert into HV format
```{bash, eval = F}
/nfs/software/birney/plink --bfile /nfs/research1/birney/users/brettell/medaka/DATA/ped_snps_only/panel_no_sibs_line_ids_snps_only_biallelic_only_no_missing_maf0.03_excl_high_het --recode HV --chr-set 24 --thin 0.025 --allele1234 --out /nfs/research1/birney/users/brettell/medaka/DATA/haploview/maf_thinned/no_high_het/panel
```

Remove asterisks
```{bash, eval = F}
cd /nfs/research1/birney/users/brettell/medaka/DATA/haploview/maf_thinned/no_high_het/
for i in `ls /nfs/research1/birney/users/brettell/medaka/DATA/haploview/maf_thinned/no_high_het/`; do sed -i 's/\*/0/g' $i; done
```

Copy to local
```{bash, eval = F}
scp brettell@ebi:/nfs/research1/birney/users/brettell/medaka/DATA/haploview/maf_thinned/no_high_het/* /Users/brettell/Documents/Repositories/medaka/WORKING/haploview/no_high_het
```

Run Haploview
```{bash, eval = F}
for i in `seq 1 24`; do java -Xmx20G -jar Documents/Software/Haploview.jar -pedfile /Users/brettell/Documents/Repositories/medaka/WORKING/haploview/no_high_het/panel.chr-$i.ped -info /Users/brettell/Documents/Repositories/medaka/WORKING/haploview/no_high_het/panel.chr-$i.info -maxDistance 1000; done
```

# Comparable human data

ALSPAC: <http://www.bristol.ac.uk/alspac/researchers/our-data/>. Consider reviewing:

* WISC - total IQ
* Crown Crisp - depression
* Suicide risk score
* Risk-taking / avoidance

*8 October 2018*

# Video analysis

Test videos here: `/nfs/ftp/private/birney-res-ftp/upload/medaka/videos/`

Tif here: `/Users/brettell/Documents/Repositories/medaka/WORKING/ilastik/-E008--PO01--LO002--CO3--SL112--PX32500--PW0100--IN0010--TM280--X077650--Y047255--Z203005--T0015570386--WE00056.tif`.

Following tutorial here: <http://ilastik.org/documentation/ilastik_manual.pdf>.

Downloaded Fiji from here (<http://imagej.net/Fiji/Downloads#Installation>) and installed on local.

Need to convert the mp4 into HDF-5 format. Downloaded HDFView to the local from here: <https://portal.hdfgroup.org/display/HDFVIEW/HDFView>.

User's guide here: <https://portal.hdfgroup.org/display/HDFVIEW/HDFView+3.x+User%27s+Guide>.



# Ideas for behavioural assays

* Boldness
- spontaneous reversal of the habenular asymmetry in zebrafish resulted in heightened boldness. So look at whether there are differences in habenular symmetry
* Preferences for males who they've observed courting other females, but even more so for males who have spawned with other females - relationship with the MHC?
* Anti-predator behaviour, i.e. shoaling or schooling - detect distances between the fish? See *Jeon et al (2015)* for methods.
* Budaev, S.V. & Andrew, R.J. (2009b) *Patterns of early embryonic light exposure determine behavioural asymmetries in zebrafish: a habenular hypothesis.* Behavioural Brain Research, 200, 91–94.
* Mental illness - depression?
* Predator inspection?
* Intelligence
- Transitive inference

***

# Install Snakemake

*6 February 2019*

## Install Miniconda first

Follow installation instructions here: <https://conda.io/projects/conda/en/latest/user-guide/install/linux.html>.

```{bash, eval = F}
# download Miniconda
wget -P /nfs/software/birney/ https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh

# install Miniconda
bash Miniconda3-latest-Linux-x86_64.sh

# Answered yes to the question whether conda shall be put into my PATH.
#Miniconda3 will now be installed into this location:
#/homes/brettell/miniconda3
```

## Install Snakemake

Follow instructions here: <https://snakemake.readthedocs.io/en/stable/getting_started/installation.html>.
```{bash, eval = F}
conda install -c bioconda -c conda-forge snakemake
```

## Download reference
```{bash, eval = F}
# make directory
mkdir /nfs/research1/birney/users/brettell/alignments/ref

# download from Ensembl (7 February 2019)
wget -P /nfs/research1/birney/users/brettell/alignments/ref/ ftp://ftp.ensembl.org/pub/release-95/fasta/oryzias_latipes/dna/Oryzias_latipes.ASM223467v1.dna.toplevel.fa.gz
```

## Instructions from Tom

Base directory:
`/nfs/research1/birney/projects/medaka/inbred_panel/medaka-alignments-release-94`

scripts:
	run_cram_to_fastq.sh
	run_bwa.sh
	run_prepare_for_gatk.sh
	run_gvcf.sh
	combine_gvcfs.sh (all outputs to be present)

Job array example - first step
`bsub -J"cramtofastq[1-115]" -M5000 -o out/out%J.%I.out "./run_cram_to_fastq.sh”`

### run_bwa.sh
```{bash, eval = F}
ALIGNPATH="../sam"
REF="/homes/tomas/birney/projects/medaka/inbred_panel/medaka-alignments-release-94/ref/Oryzias_latipes.ASM223467v1.dna.toplevel.fa.gz"
BWA="/nfs/software/birney/bwa/bwa mem -p -M -t 1  $REF"
DIRECT="../fastq"
FILES=($(ls $DIRECT ))
f=${FILES[$LSB_JOBINDEX-1]}
fbname=$(basename "$f" .fastq)
COMMAND="$BWA $DIRECT/$f > $ALIGNPATH/$fbname.sam"

eval "$COMMAND"
```

Tool docs here: <http://bio-bwa.sourceforge.net/bwa.shtml>.

mem: Align usign mem algorithm.
-p: Assume the first input query file in interleaved paired-end FASTA/Q.

```{python}
from os.path import join

# Globals ––––––––––––––––––––––––––––––

DATA_DIR = "/homes/tomas/birney/projects/medaka/inbred_panel/medaka-alignments-release-94/"
SAMPLE, = glob_wildcards(join(DATA_DIR, "fastq/{sample}.fastq"))
REF = "/nfs/research1/birney/users/brettell/alignments/ref/Oryzias_latipes.ASM223467v1.dna.toplevel.fa.gz"
BWA = "/nfs/software/birney/bwa/bwa"

# Rules ––––––––––––––––––––––––––––––––

rule all:
  input:
      expand("mapped/{sample}.bam", sample = SAMPLE)

rule bwa_mem:
  input:
      reads=[join(DATA_DIR, "fastq/{sample}.fastq")]
  output:
      "mapped/{sample}.bam"
  log:
      "logs/bwa_mem/{sample}.log"
  params:
      index= REF,
      extra=r"-R '@RG\tID:{sample}\tSM:{sample}'",
      sort="none",             # Can be 'none', 'samtools' or 'picard'.
      sort_order="queryname",  # Can be 'queryname' or 'coordinate'.
      sort_extra=""            # Extra args for samtools/picard.
  threads: 8
  wrapper:
      "0.31.1/bio/bwa/mem"
```

```{python}
from os.path import join

# Globals ––––––––––––––––––––––––––––––

DATA_DIR = "/homes/tomas/birney/projects/medaka/inbred_panel/medaka-alignments-release-94/"
SAMPLE = "22013_2#1"
REF = "/nfs/research1/birney/users/brettell/alignments/ref/Oryzias_latipes.ASM223467v1.dna.toplevel.fa.gz"
BWA = "/nfs/software/birney/bwa/bwa"

# Rules ––––––––––––––––––––––––––––––––

rule all:
  input:
      expand("mapped/{sample}.bam", sample = SAMPLE)

rule bwa_mem:
  input:
      reads=join(DATA_DIR, "fastq/{sample}.fastq")
  output:
      "mapped/{sample}.bam"
  log:
      "logs/bwa_mem/{sample}.log"
  params:
      index=REF,
      extra=r"-R '@RG\tID:{sample}\tSM:{sample}'",
      sort="none",             # Can be 'none', 'samtools' or 'picard'.
      sort_order="queryname",  # Can be 'queryname' or 'coordinate'.
      sort_extra=""            # Extra args for samtools/picard.
  threads: 8
  wrapper:
      "0.31.1/bio/bwa/mem"
```

```{bash, eval = F}
#TEST BWA CODE
/nfs/software/birney/bwa/bwa mem -t 1 -R '@RG\tID:22013_2#1\tSM:22013_2#1' /nfs/research1/birney/users/brettell/alignments/ref/Oryzias_latipes.ASM223467v1.dna.toplevel.fa.gz /homes/tomas/birney/projects/medaka/inbred_panel/medaka-alignments-release-94/fastq/22013_2#1.fastq
# [E::bwa_idx_load_from_disk] fail to locate the index files
```

### Unzip and rename reference so we can use with the wrapper
```{bash, eval = F}
#unzip
gunzip /nfs/research1/birney/users/brettell/alignments/ref/Oryzias_latipes.ASM223467v1.dna.toplevel.fa.gz
#rename
mv /nfs/research1/birney/users/brettell/alignments/ref/Oryzias_latipes.ASM223467v1.dna.toplevel.fa /nfs/research1/birney/users/brettell/alignments/ref/Oryzias_latipes.ASM223467v1.dna.toplevel.fasta

```

### Index genome

#### TEST ON BASH
```{bash, eval = F}
/nfs/software/birney/bwa/bwa index -p /nfs/research1/birney/users/brettell/alignments/ref/Oryzias_latipes.ASM223467v1.dna.toplevel /nfs/research1/birney/users/brettell/alignments/ref/Oryzias_latipes.ASM223467v1.dna.toplevel.fa.gz
# WORKS!... but doesn't create .sa file?
```

#### Snakemake script
```{python, eval = F}
from os.path import join

# Globals ––––––––––––––––––––––––––––––

SW_DIR = "/nfs/software/birney/"
DATA_DIR = "/homes/tomas/birney/projects/medaka/inbred_panel/medaka-alignments-release-94/"
SAMPLE, = glob_wildcards(join(DATA_DIR, "fastq/{sample}.fastq"))
REF_PREF = "/nfs/research1/birney/users/brettell/alignments/ref/Oryzias_latipes.ASM223467v1.dna.toplevel"
BWA = join(SW_DIR, "bwa/bwa")

# Rules ––––––––––––––––––––––––––––––––

rule bwa_index:
    input:
        "{REF_PREF}.fa.gz"
    output:
        "{REF_PREF}.amb",
        "{REF_PREF}.ann",
        "{REF_PREF}.bwt",
        "{REF_PREF}.pac",
        "{REF_PREF}.sa"
    shell:
        "{BWA} index -p {REF_PREF} {REF_PREF}.fa.gz"
```

#### TEST ON SNAKEMAKE
```{bash, eval = F}
snakemake --cluster "bsub -M 10000" --jobs 200 -p /nfs/research1/birney/users/brettell/alignments/ref/Oryzias_latipes.ASM223467v1.dna.toplevel.fa.gz.amb /nfs/research1/birney/users/brettell/alignments/ref/Oryzias_latipes.ASM223467v1.dna.toplevel.fa.gz.ann /nfs/research1/birney/users/brettell/alignments/ref/Oryzias_latipes.ASM223467v1.dna.toplevel.fa.gz.bwt /nfs/research1/birney/users/brettell/alignments/ref/Oryzias_latipes.ASM223467v1.dna.toplevel.fa.gz.pac /nfs/research1/birney/users/brettell/alignments/ref/Oryzias_latipes.ASM223467v1.dna.toplevel.fa.gz.sa
# SUCCESS!
```

### Align

#### Test on bash

```{bash, eval = F}
bsub -M 10000 -Is bash
/nfs/software/birney/bwa/bwa mem -p -M -t 1 /nfs/research1/birney/users/brettell/alignments/ref/Oryzias_latipes.ASM223467v1.dna.toplevel.fa.gz /homes/tomas/birney/projects/medaka/inbred_panel/medaka-alignments-release-94/fastq/22013_2#1.fastq > mapped/22013_2#1.bam
# WORKS
```

#### Snakemake code
```{python, eval = F}
from os.path import join

# Globals                              

SW_DIR = "/nfs/software/birney/"
DATA_DIR = "/homes/tomas/birney/projects/medaka/inbred_panel/medaka-alignments-release-94/"
SAMPLES, = glob_wildcards(join(DATA_DIR, "fastq/{sample}.fastq"))
REF = "/nfs/research1/birney/users/brettell/alignments/ref/Oryzias_latipes.ASM223467v1.dna.toplevel.fa.gz"
BWA = join(SW_DIR, "bwa/bwa")

# Rules                                

rule all:
    input:
        expand("mapped/{sample}.bam", sample = SAMPLES)

rule bwa_index:
    input:
        "{REF}"
    output:
        "{REF}.amb",
        "{REF}.ann",
        "{REF}.bwt",
        "{REF}.pac",
        "{REF}.sa"
    shell:
        "{BWA} index -p {REF} {REF}"

rule bwa_mem:
    input:
        join(DATA_DIR, "fastq/{sample}.fastq")
    output:
        "mapped/{sample}.bam"
    shell:
        "{BWA} mem -p -M -t 1 {REF} {input} > {output}"
```

#### Run
```{bash, eval = F}
snakemake --cluster "bsub -M 10000" --jobs 200 -p
```

The samples range in size from 48-90GB. Some samples have anomalous sizes, namely:
* 22013_7#2: 9,8GB
* 22024_4#5: 15GB.
But it's the same in Tom's? Leave it for now...

But then they both threw up errors in the next step (MarkDuplicates). Run independently.
```{bash, eval = F}
# 22013_7#2
bsub -M 10000 /nfs/software/birney/bwa/bwa mem -p -M -t 1 /nfs/research1/birney/users/brettell/alignments/ref/Oryzias_latipes.ASM223467v1.dna.toplevel.fa.gz /homes/tomas/birney/projects/medaka/inbred_panel/medaka-alignments-release-94/fastq/22013_7#2.fastq > /nfs/research1/birney/users/brettell/mapped/22013_7#2.bam


```

```{python}
from os.path import join

# Globals                              

SW_DIR = "/nfs/software/birney/"
DATA_DIR = "/homes/tomas/birney/projects/medaka/inbred_panel/medaka-alignments-release-94/"
SAMPLES, = glob_wildcards(join(DATA_DIR, "fastq/{sample}.fastq"))
REF = "/nfs/research1/birney/users/brettell/alignments/ref/Oryzias_latipes.ASM223467v1.dna.toplevel.fa.gz"
BWA = join(SW_DIR, "bwa/bwa")

# Rules                                

rule all:
    input:
        "mapped/22013_7#2.bam",
        "mapped/22024_4#5.bam"

rule bwa_index:
    input:
        "{REF}"
    output:
        "{REF}.amb",
        "{REF}.ann",
        "{REF}.bwt",
        "{REF}.pac",
        "{REF}.sa"
    shell:
        "{BWA} index -p {REF} {REF}"

rule bwa_mem:
    input:
        join(DATA_DIR, "fastq/{sample}.fastq")
    output:
        "mapped/{sample}.bam"
    shell:
        "{BWA} mem -p -M -t 1 {REF} {input} > {output}"
```

```{bash, eval = F}
snakemake --cluster "bsub -M 10000" --jobs 200 -p -R bwa_mem
#7964669
#7964673
```


### Mark duplicates

Tom's code here: `/nfs/research1/birney/projects/medaka/inbred_panel/medaka-alignments-release-94/scripts/run_prepare_for_gatk.sh`.

```{bash, eval = F}
ALIGNPATH="../sam"
PICARD="java -Xmx4g -Djava.io.tmpdir=/tmp -jar /nfs/software/birney/picard.jar SortSam SO=coordinate "
BAMPATH="../bam"
FILES=($(ls $ALIGNPATH ))
f=${FILES[$LSB_JOBINDEX-1]}
fbname=$(basename "$f" .sam)
COMMAND="$PICARD INPUT=$ALIGNPATH/$f OUTPUT=$BAMPATH/$fbname.bam VALIDATION_STRINGENCY=LENIENT CREATE_INDEX=false"


if [ -f $BAMPATH/$fbname.bai ]; then
   echo "File $fbname.bai exists."
else
	eval "$COMMAND"

	DEDUP="java -jar /nfs/software/birney/picard.jar MarkDuplicates "

	COMMAND="$DEDUP I=$BAMPATH/$fbname.bam "
	COMMAND="$COMMAND O=$BAMPATH/$fbname.t.bam "
	eval "$COMMAND"

	COMMAND="mv $BAMPATH/$fbname.t.bam $BAMPATH/$fbname.bam"
	eval "$COMMAND"

	ADDREPLACEREADGREOUPS="java -jar /nfs/software/birney/picard.jar AddOrReplaceReadGroups "

	IFS='# ' read -r -a name_items <<< "$f"
	IFS='_ ' read -r -a items <<< "${name_items[0]}"

	COMMAND="$ADDREPLACEREADGREOUPS I=$BAMPATH/$fbname.bam "
	COMMAND="$COMMAND O=$BAMPATH/$fbname.t.bam "
	COMMAND="$COMMAND RGID=1 "
	COMMAND="$COMMAND RGLB=DN474009U "
	COMMAND="$COMMAND RGPL=illumina "
	COMMAND="$COMMAND RGPU=${items[0]} "
	COMMAND="$COMMAND RGSM=$fbname"

	eval "$COMMAND"

	COMMAND="mv $BAMPATH/$fbname.t.bam $BAMPATH/$fbname.bam"
	eval "$COMMAND"

	COMMAND="java -jar /nfs/software/birney/picard.jar BuildBamIndex "
	COMMAND="$COMMAND I=$BAMPATH/$fbname.bam"
	eval "$COMMAND"
fi
```

**Note**: For AddOrReplaceReadGroups, the RGPU input is the five-digit sequencing ID, e.g. 22013. Ascertained with the following code:
```{bash, eval = F}
cd /nfs/research1/birney/projects/medaka/inbred_panel/medaka-alignments-release-94/scripts/
f=../sam/22013_2#1.sam
fbname=$(basename "$f" .sam)
echo $fbname
# 22013_2#1
echo ${items[0]}
# ../sam/22013
```

`/nfs/research1/birney/users/brettell/alignment/cluster.json`
```{bash, eval = F}
{
    "__default__" :
    {
        "memory" : "50000",
        "n" : "1",
        "resources" : "\"select[mem>50000] rusage[mem=50000] span[hosts=1]\"",
        "queue" : "research-rh7",
        "name" : "{rule}.{wildcards}",
        "output" : "log/{rule}.out",
        "error" : "log/{rule}.err"
    }
}  
```

Bash command.
```{bash, eval = F}
snakemake -s alignment/al.smk --jobs 5000 --latency-wait 300 --cluster-config alignment/cluster.json --cluster 'bsub -g /snakemake_bgenie -J {cluster.name} -q {cluster.queue} -n {cluster.n} -R {cluster.resources} -M {cluster.memory} -o {cluster.output} -e {cluster.error}' --keep-going --rerun-incomplete --use-conda -p
```


#### SortSam

Tried to run MarkDuplicates first, but it kept failing. Not sure why. Change locations of reference files to `/nfs/research1/birney/users/brettell/ref/`.
```{python, eval = F}
from os.path import join

# Globals                              

SW_DIR = "/nfs/software/birney/"
DATA_DIR = "/homes/tomas/birney/projects/medaka/inbred_panel/medaka-alignments-release-94/"
SAMPLES, = glob_wildcards(join(DATA_DIR, "fastq/{sample}.fastq"))
REF = "/nfs/research1/birney/users/brettell/ref/Oryzias_latipes.ASM223467v1.dna.toplevel.fa.gz"
BWA = join(SW_DIR, "bwa/bwa")
PICARD = join(SW_DIR, "picard.jar")

# Rules                                

rule all:
    input:
        expand("marked/sorted/{sample}.bam", sample = SAMPLES)

rule bwa_index:
    input:
        "{REF}"
    output:
        "{REF}.amb",
        "{REF}.ann",
        "{REF}.bwt",
        "{REF}.pac",
        "{REF}.sa"
    shell:
        "{BWA} index -p {REF} {REF}"

rule bwa_mem:
    input:
        join(DATA_DIR, "fastq/{sample}.fastq")
    output:
        "mapped/{sample}.bam"
    shell:
        "{BWA} mem -p -M -t 1 {REF} {input} > {output}"

rule sort_sam:
    input:
        "mapped/{sample}.bam"
    output:
        "marked/sorted/{sample}.bam"
    shell:
        "java -Xmx4g -Djava.io.tmpdir=/tmp -jar {PICARD} SortSam SO=coordinate INPUT={input} OUTPUT={output} VALIDATION_STRINGENCY=LENIENT CREATE_INDEX=false"

# SUCCESS! Although a number of jobs failed for an unknown reason, and needed to be run again.
```

MarkDuplicates is failing on 14 samples. Try creating indexes.
```{python}
from os.path import join

# Globals                              

SW_DIR = "/nfs/software/birney/"
DATA_DIR = "/homes/tomas/birney/projects/medaka/inbred_panel/medaka-alignments-release-94/"
SAMPLES, = glob_wildcards(join(DATA_DIR, "fastq/{sample}.fastq"))
REF = "/nfs/research1/birney/users/brettell/ref/Oryzias_latipes.ASM223467v1.dna.toplevel.fa.gz"
BWA = join(SW_DIR, "bwa/bwa")
PICARD = join(SW_DIR, "picard.jar")

# Rules                                

rule all:
    input:
        expand("marked/sorted/{sample}.bam", sample = SAMPLES),
        expand("marked/sorted/{sample}.bai", sample = SAMPLES)

rule bwa_index:
    input:
        "{REF}"
    output:
        "{REF}.amb",
        "{REF}.ann",
        "{REF}.bwt",
        "{REF}.pac",
        "{REF}.sa"
    shell:
        "{BWA} index -p {REF} {REF}"

rule bwa_mem:
    input:
        join(DATA_DIR, "fastq/{sample}.fastq")
    output:
        "mapped/{sample}.bam"
    shell:
        "{BWA} mem -p -M -t 1 {REF} {input} > {output}"

rule sort_sam:
    input:
        "mapped/{sample}.bam"
    output:
        bam = "marked/sorted/{sample}.bam",
        index = "marked/sorted/{sample}.bai"
    shell:
        "java -Xmx4g -Djava.io.tmpdir=/tmp -jar {PICARD} SortSam SO=coordinate INPUT={input} OUTPUT={output.bam} VALIDATION_STRINGENCY=LENIENT CREATE_INDEX=true"
```


#### MarkDuplicates

##### Test on bash

```{bash, eval = F}
cd /nfs/research1/birney/users/brettell/
mkdir /nfs/research1/birney/users/brettell/marked
bsub -M 10000 -Is bash
java -Xmx4g -jar /nfs/software/birney/picard.jar MarkDuplicates \
  I=mapped/22013_2#1.bam \
  O=marked/22013_2#1.bam \
  M=marked/22013_2#1_metrics.txt

java -Xmx4g -jar /nfs/software/birney/picard.jar MarkDuplicates \I=mapped/22013_2#1.bam \O=marked/22013_2#1.bam \M=marked/22013_2#1_metrics.txt

java -Xmx4g -jar /nfs/software/birney/picard.jar  MarkDuplicates \I=mapped/22218_7#1.bam \O=marked/marked_duplicates_22218_7#1.bam \M=marked/marked_dup_metrics_22218_7#1.txt
# WORKS
```

##### Snakemake script
```{python, eval = F}
from os.path import join

# Globals                              

SW_DIR = "/nfs/software/birney/"
DATA_DIR = "/homes/tomas/birney/projects/medaka/inbred_panel/medaka-alignments-release-94/"
SAMPLES, = glob_wildcards(join(DATA_DIR, "fastq/{sample}.fastq"))
REF = "/nfs/research1/birney/users/brettell/ref/Oryzias_latipes.ASM223467v1.dna.toplevel.fa.gz"
BWA = join(SW_DIR, "bwa/bwa")
PICARD = join(SW_DIR, "picard.jar")

# Rules                                

rule all:
    input:
        expand("marked/marked/{sample}.bam", sample = SAMPLES),
        expand("marked/marked/{sample}.txt", sample = SAMPLES)

rule bwa_index:
    input:
        "{REF}"
    output:
        "{REF}.amb",
        "{REF}.ann",
        "{REF}.bwt",
        "{REF}.pac",
        "{REF}.sa"
    shell:
        "{BWA} index -p {REF} {REF}"

rule bwa_mem:
    input:
        join(DATA_DIR, "fastq/{sample}.fastq")
    output:
        "mapped/{sample}.bam"
    shell:
        "{BWA} mem -p -M -t 1 {REF} {input} > {output}"

rule sort_sam:
    input:
        "mapped/{sample}.bam"
    output:
        "marked/sorted/{sample}.bam"
    shell:
        "java -Xmx4g -Djava.io.tmpdir=/tmp -jar {PICARD} SortSam SO=coordinate INPUT={input} OUTPUT={output} VALIDATION_STRINGENCY=LENIENT CREATE_INDEX=false"

rule picard_mark_duplicates:
    input:
        "marked/sorted/{sample}.bam"
    output:
        bam = "marked/marked/{sample}.bam",
        metrics = "marked/marked/{sample}.txt"
    shell:
        "java -Xmx4g -jar {PICARD} MarkDuplicates I={input} O={output.bam} M={output.metrics}"

```

##### Run
```{bash, eval = F}
snakemake --cluster "bsub -M 10000" --jobs 200 -p
```

#### Test on bash
```{bash, eval = F}
cd /nfs/research1/birney/users/brettell/
bsub -M 10000 -Is bash
java -Xmx4g -jar /nfs/software/birney/picard.jar SortSam \
  I=mapped/22013_2#1.bam \
  O=sorted//22013_2#1.bam
```

####

##### Test on bash

```{bash, eval = F}
cd /nfs/research1/birney/users/brettell/
bsub -M 10000 -Is bash
java -jar /nfs/software/birney/picard-2.9.0/picard.jar FastqToSam \

```


### run_prepare_for_gatk.sh
```{bash, eval = F}

```


## Move fastq files to home drive and change names
```{bash, eval = F}
# create directory for files
mkdir /nfs/research1/birney/users/brettell/alignments
mkdir /nfs/research1/birney/users/brettell/alignments/fastq

# print all cram-line keys
tail -n +2 /nfs/research1/birney/users/brettell/medaka/DATA/cram_file_to_line_ids.txt

# copy all to own directory
tail -n +2 /nfs/research1/birney/users/brettell/medaka/DATA/cram_file_to_line_ids.txt | while read n k; do cp /nfs/research1/birney/projects/medaka/inbred_panel/medaka-alignments-release-94/fastq/$n.fastq /nfs/research1/birney/users/brettell/alignments/fastq/$k.fastq; done
# works, but takes too long (each file is ~50GB). Just use the files from Tomas's directory and change the file names later.
```





## Install latest version of GATK (v4.1.0.0)

*7 February 2019*

```{bash, eval = F}
cd /nfs/software/birney
# download and unzip
wget https://github.com/broadinstitute/gatk/releases/download/4.1.0.0/gatk-4.1.0.0.zip
unzip gatk-4.1.0.0.zip

# add to path

```

## Run Snakemake file

Snakefile here: `/nfs/research1/birney/users/brettell/Snakefile`
```{bash, eval = F}

```


# Carry out new analysis on current ensembl95 VCF
```{bash, eval = F}
#

# move to my directory
mkdir /nfs/research1/birney/users/brettell/vcf

bsub gzip -c /homes/tomas/birney/projects/medaka/inbred_panel/medaka-alignments-release-94/vcf/medaka_inbred_panel_ensembl_new_reference_release_94.vcf > /nfs/research1/birney/users/brettell/vcf/kiyosu_panel.vcf.gz
```

*14 February 2019*

* Ran idTracker on the provided video (500MB). Segmentation step took nearly 1h.

*19 February 2019*

# Analyse LD and inbreeding in panel

## LD analysis

Snakemake file here: `/nfs/research1/birney/users/brettell/ld.smk`

Config file here: `/nfs/research1/birney/users/brettell/ld_config.json`

Bash script
```{bash, eval = F}
cd /nfs/research1/birney/users/brettell

snakemake -s ld.smk --jobs 5000 --latency-wait 300 --cluster-config ld_cluster.json --cluster 'bsub -g /snakemake_bgenie -J {ld_cluster.name} -q {ld_cluster.queue} -n {ld_cluster.n} -R {ld_cluster.resources} -M {ld_cluster.memory} -o {ld_cluster.output} -e {ld_cluster.error}' --keep-going --rerun-incomplete --use-conda -p
#RuleException in line 14 of /nfs/research1/birney/users/brettell/ld.smk:
#NameError: The name 'ld_cluster' is unknown in this context. Please make sure that you defined that variable. Also note that braces not used for variable access have to be escaped by repeating them, i.e. {{print $1}}

# Does it only work if the cluster config file is called "cluster.json"?
snakemake -s ld.smk --jobs 5000 --latency-wait 300 --cluster-config cluster.json --cluster 'bsub -g /snakemake_bgenie -J {cluster.name} -q {cluster.queue} -n {cluster.n} -R {cluster.resources} -M {cluster.memory} -o {cluster.output} -e {cluster.error}' --keep-going --rerun-incomplete --use-conda -p
# YES.

# Try it with "_ld" after "cluster"
snakemake -s ld.smk --jobs 5000 --latency-wait 300 --cluster-config cluster_ld.json --cluster 'bsub -g /snakemake_bgenie -J {cluster_ld.name} -q {cluster_ld.queue} -n {cluster_ld.n} -R {cluster_ld.resources} -M {cluster_ld.memory} -o {cluster_ld.output} -e {cluster_ld.error}' --keep-going --rerun-incomplete --use-conda -p
#RuleException in line 14 of /nfs/research1/birney/users/brettell/ld.smk:
#NameError: The name 'cluster_ld' is unknown in this context. Please make sure that you defined that variable. Also note that braces not used for variable access have to be escaped by repeating them, i.e. {{print $1}}
```

Will have to silo each analysis into its own working directory. More tidy that way anyway.
```{bash, eval = F}
mkcd /nfs/research1/birney/users/brettell/ld_analysis
# create cluster and snakemake files in this directory, then:

cd..

snakemake -s ld_analysis/ld.smk --jobs 5000 --latency-wait 300 --cluster-config ld_analysis/cluster.json --cluster 'bsub -g /snakemake_bgenie -J {cluster.name} -q {cluster.queue} -n {cluster.n} -R {cluster.resources} -M {cluster.memory} -o {cluster.output} -e {cluster.error}' --keep-going --rerun-incomplete --use-conda -p
```

Cluster config
```{bash, eval = F}
{
    "__default__" :
    {
        "memory" : "50000",
        "n" : "1",
        "resources" : "\"select[mem>50000] rusage[mem=50000] span[hosts=1]\"",
        "queue" : "research-rh7",
        "name" : "{rule}.{wildcards}",
        "output" : "log/{rule}.out",
        "error" : "log/{rule}.err"
    }
}    
```

### Remove siblings from VCF

#### Zip and copy VCF to working directory

Snakemake file
```{python, eval = F}
from os.path import join

# Globals                              

VCF = "/nfs/research1/birney/projects/medaka/inbred_panel/medaka-alignments-release-94/vcf/medaka_inbred_panel_ensembl_new_reference_release_94.vcf"
SW_DIR = "/nfs/software/birney/"

# Rules                                

rule all:
    input:
        "vcf/kiyosu.vcf.gz"

rule cp_vcf:
    input:
        expand("{vcf}", vcf = VCF)
    output:
        "vcf/kiyosu.vcf.gz"
    shell:
        "gzip -c {input} > {output}"
# 9997938
```


#### Replace sample names with line IDs

Check sample names line up with the comparison file
```{bash, eval = F}
# print sample names
/nfs/software/birney/bcftools/bcftools query -l vcf/kiyosu_panel.vcf.gz  > tmp1.txt

# compare with first column of line IDs file (sample IDs)
cut -f1 /nfs/research1/birney/users/brettell/medaka/DATA/cram_file_to_line_ids.txt | tail -n +2 > tmp2.txt

diff tmp1.txt tmp2.txt
# no output, therefore the same.

# rm temp files
rm tmp*

# create file with just line IDs
cut -f 2 /nfs/research1/birney/users/brettell/medaka/DATA/cram_file_to_line_ids.txt | tail -n +2 > /nfs/research1/birney/users/brettell/medaka/DATA/line_ids.txt
```

Snakemake file
```{python, eval = F}
from os.path import join

# Globals                              

VCF = "/nfs/research1/birney/projects/medaka/inbred_panel/medaka-alignments-release-94/vcf/medaka_inbred_panel_ensembl_new_reference_release_94.vcf"
SW_DIR = "/nfs/software/birney/"
BCF = join(SW_DIR, "bcftools/bcftools")
LINE_IDS = "/nfs/research1/birney/users/brettell/medaka/DATA/line_ids.txt"

# Rules                                

rule all:
    input:
        "vcf/kiyosu_line_ids.vcf"

rule copy_vcf:
    input:
        expand("{vcf}", vcf = VCF)
    output:
        "vcf/kiyosu.vcf.gz"
    shell:
        "gzip -c {input} > {output}"

rule replace_sample_ids:
    input:
        "vcf/kiyosu.vcf.gz"
    output:
        "vcf/kiyosu_line_ids.vcf"
    shell:
        "{BCF} reheader --samples {LINE_IDS} {input} > {output}"


```

**Note**: Trying to rename the output as the input made the job fail, so avoid in the future.

**Note**: Trying to output BCF tools as a .gz file seemed to create an error (where only 81 lines could be read). That's why we're outputing to an uncompressed VCF.

# Missing tail mutant analysis

*26 February 2019*

Hi Ian,

We have a side project with Felix and thought that it would make a nice piece of analysis for you.
- i think it would be a good additional piece of work while your setting up the behavioural assays

One would hope that this results in some kind of manuscript and it should be a nice addition for your year 1 TAC.

These are the details:
-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Felix has been keeping 3 mutant lines (missing tail phenotype, embryonic lethal) in a pure iCab background (for 3 generations).
- mutants are STS, SOS, and KPK

He has then crossed these mutants into a Kaga background using a F2 segregation analysis of iCab/Kaga F1 carrier intercross.
-i.e,  selecting the missing tail phenotype embryos and crossing into F2

50 phenotypically mutant embryos were then pooled for each mutant (STS, SOS and KPK).
- each was sequenced 4 times on a HiSeq (replicates)

We have these data files with us now (its basically 12 datasets, 4 replicates for each mutant).

The basic idea would be to align these to HdrR and using the iCab - Kaga divergent genotypes search for regions of iCab homozygosity.
- create a map of iCab homozygosity and within the regions search for loss of function mutations

Felix has already done some PCR based mapping and has some candidate regions.

-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

Are you interested is this?

If so i think the first thing to do is get these datasets aligned to the HdrR reference, call variants and annotate those variants.

Then you would need to create a map of iCab homozygous across the 3 mutants and search for loss of function variants in those regions.
- we can discuss the details here later

Cheers,

Tom

## Handy code

```{bash, eval = F}
# prints all jobs and terminates
bjobs | cut -f1 -d' ' | tail -n +2 | sed ':a;N;$!ba;s/\n/ /g' | xargs bkill
```


## Relevant directories

`/nfs/research1/birney/projects/medaka/deepseqlab`: contains 111G of 24 fastq files (3 mutants with paired reads in 2 separate files, 2 replicates per lane, 2 lanes).

Reference: `/nfs/research1/birney/projects/medaka/inbred_panel/medaka-alignments-release-94/ref/Oryzias_latipes.ASM223467v1.dna.toplevel.fa`

```{bash, eval = F}
home # alias  = cd /nfs/research1/birney/users/brettell/
mkcd tailess
mkdir log
```

## Reference preparation
```{bash, eval = F}
bsub -Is bash
# unzip to create .FAI file (for some reason it didn't work if it's zipped?)
gunzip -c /nfs/research1/birney/users/brettell/ref/Oryzias_latipes.ASM223467v1.dna.toplevel.fa.gz > /nfs/research1/birney/users/brettell/ref/Oryzias_latipes.ASM223467v1.dna.toplevel.fa
# create .FAI file
/nfs/software/birney/samtools-1.9/samtools faidx /nfs/research1/birney/users/brettell/ref/Oryzias_latipes.ASM223467v1.dna.toplevel.fa
# create .DICT file
java -jar /nfs/software/birney/picard.jar CreateSequenceDictionary R=/nfs/research1/birney/users/brettell/ref/Oryzias_latipes.ASM223467v1.dna.toplevel.fa O=/nfs/research1/birney/users/brettell/ref/Oryzias_latipes.ASM223467v1.dna.toplevel.dict
```

## Cluster config

In `/nfs/research1/birney/users/brettell/tailess/cluster.json`

```{bash, eval = F}
{
    "__default__" :
    {
        "memory" : "50000",
        "n" : "1",
        "resources" : "\"select[mem>50000] rusage[mem=50000] span[hosts=1]\"",
        "queue" : "research-rh7",
        "name" : "{rule}.{wildcards}",
        "output" : "log/{rule}_{wildcards}.out",
        "error" : "log/{rule}_{wildcards}.err"
    },
    "bwa_mem_1" :
    {
        "n" : "16"
    },
    "bwa_mem_2" :
    {
        "n" : "16"
    },
    "picard_sort_sam" :
    {
        "n" : "4"
    },
    "samtools_sort_sam" :
    {
        "n" : "2",
        "output" : "log/test/{rule}_{wildcards}.out",
        "error" : "log/test/{rule}_{wildcards}.err"
    },
    "picard_mark_duplicates" :
    {
        "n" : "8"
    }
}
```

## Bash script

```{bash, eval = F}
snakemake --jobs 5000 --latency-wait 300 --cluster-config cluster.json --cluster 'bsub -g /snakemake_bgenie -J {cluster.name} -q {cluster.queue} -n {cluster.n} -R {cluster.resources} -M {cluster.memory} -o {cluster.output} -e {cluster.error}' --keep-going --rerun-incomplete --use-conda -pn
```

## Snakemake

`/nfs/research1/birney/users/brettell/tailess/Snakefile`

### Align with BWA-MEM

#### TEST

```{bash, eval = F}
mkdir /nfs/research1/birney/users/brettell/tailess/mapped/

/nfs/software/birney/bwa/bwa mem -M -t 1 /nfs/research1/birney/users/brettell/ref/Oryzias_latipes.ASM223467v1.dna.toplevel.fa.gz /nfs/research1/birney/projects/medaka/deepseqlab/H2YM2BBXY_MC259_19s000097-1-1_Ibberson_lane5KPK1_1_sequence.txt.gz /nfs/research1/birney/projects/medaka/deepseqlab/H2YM2BBXY_MC259_19s000097-1-1_Ibberson_lane5KPK1_2_sequence.txt.gz > /nfs/research1/birney/users/brettell/tailess/mapped/5KPK1.bam

# seems to work
```


#### TRUE

```{python, eval = F}
from os.path import join

# Globals                              

SW_DIR = "/nfs/software/birney/"
DATA_DIR = "/nfs/research1/birney/projects/medaka/deepseqlab/"
(LANE_1, LANE_2, SAMPLES, PAIRS) = glob_wildcards(join(DATA_DIR, "H2YM2BBXY_MC259_19s000097-{lane_1}-1_Ibberson_lane{lane_2,\d+}{sample}_{pair}_sequence.txt.gz"))
REF = "/nfs/research1/birney/users/brettell/ref/Oryzias_latipes.ASM223467v1.dna.toplevel.fa.gz"
BWA = join(SW_DIR, "bwa/bwa")

# Rules                                

rule all:
    input:
        expand("mapped/{lane_1}_{sample}.bam", sample = SAMPLES, lane_1 = LANE_1)

rule bwa_mem_1:
    input:
        l1_r1 = join(DATA_DIR, "H2YM2BBXY_MC259_19s000097-1-1_Ibberson_lane5{sample}_1_sequence.txt.gz"),
        l1_r2 = join(DATA_DIR, "H2YM2BBXY_MC259_19s000097-1-1_Ibberson_lane5{sample}_2_sequence.txt.gz")
    output:
        "mapped/1_{sample}.bam"
    shell:
        "{BWA} mem -M -t 16 {REF} {input.l1_r1} {input.l1_r2} > {output}"

rule bwa_mem_2:
    input:
        l2_r1 = join(DATA_DIR, "H2YM2BBXY_MC259_19s000097-2-1_Ibberson_lane6{sample}_1_sequence.txt.gz"),
        l2_r2 = join(DATA_DIR, "H2YM2BBXY_MC259_19s000097-2-1_Ibberson_lane6{sample}_2_sequence.txt.gz")
    output:
        "mapped/2_{sample}.bam"
    shell:
        "{BWA} mem -M -t 16 {REF} {input.l2_r1} {input.l2_r2} > {output}"  

#SUCCESS        
```

### SortSam

#### TEST

```{bash, eval = F}
java -Xmx4g -Djava.io.tmpdir=/tmp -jar /nfs/software/birney/picard.jar SortSam SO=coordinate INPUT=mapped/1_KPK1.bam OUTPUT=sorted/test.bam VALIDATION_STRINGENCY=LENIENT CREATE_INDEX=true
# WORKS
```

#### TRUE
```{python}
from os.path import join

# Globals                              

SW_DIR = "/nfs/software/birney/"
DATA_DIR = "/nfs/research1/birney/projects/medaka/deepseqlab/"
(LANE_1, LANE_2, SAMPLES, PAIRS) = glob_wildcards(join(DATA_DIR, "H2YM2BBXY_MC259_19s000097-{lane_1}-1_Ibberson_lane{lane_2,\d+}{sample}_{pair}_sequence.txt.gz"))
REF = "/nfs/research1/birney/users/brettell/ref/Oryzias_latipes.ASM223467v1.dna.toplevel.fa.gz"
BWA = join(SW_DIR, "bwa/bwa")
PICARD = join(SW_DIR, "picard.jar")

# Rules                                

rule all:
    input:
        expand("sorted/{lane_1}_{sample}.bam", sample = SAMPLES, lane_1 = LANE_1),
        expand("sorted/{lane_1}_{sample}.bai", sample = SAMPLES, lane_1 = LANE_1)

rule bwa_mem_1:
    input:
        l1_r1 = join(DATA_DIR, "H2YM2BBXY_MC259_19s000097-1-1_Ibberson_lane5{sample}_1_sequence.txt.gz"),
        l1_r2 = join(DATA_DIR, "H2YM2BBXY_MC259_19s000097-1-1_Ibberson_lane5{sample}_2_sequence.txt.gz")
    output:
        "mapped/1_{sample}.bam"
    shell:
        "{BWA} mem -M -t 16 {REF} {input.l1_r1} {input.l1_r2} > {output}"

rule bwa_mem_2:
    input:
        l2_r1 = join(DATA_DIR, "H2YM2BBXY_MC259_19s000097-2-1_Ibberson_lane6{sample}_1_sequence.txt.gz"),
        l2_r2 = join(DATA_DIR, "H2YM2BBXY_MC259_19s000097-2-1_Ibberson_lane6{sample}_2_sequence.txt.gz")
    output:
        "mapped/2_{sample}.bam"
    shell:
        "{BWA} mem -M -t 16 {REF} {input.l2_r1} {input.l2_r2} > {output}"

rule picard_sort_sam:
    input:
        "mapped/{lane_1}_{sample}.bam"
    output:
        bam = "sorted/{lane_1}_{sample}.bam",
        bai = "sorted/{lane_1}_{sample}.bai"
    shell:
        "java -Xmx4g -Djava.io.tmpdir=/tmp -jar {PICARD} SortSam SO=coordinate INPUT={input} OUTPUT={output.bam} VALIDATION_STRINGENCY=LENIENT CREATE_INDEX=true"

```

### MarkDuplicates

**NOTE**: Received the following error messagen when trying to run MarkDuplicates before SortSam: `Exception in thread "main" picard.PicardException: This program requires input that are either coordinate or query sorted. Found unsorted`.

#### TEST

```{bash}
java -Xmx4g -jar /nfs/software/birney/picard.jar MarkDuplicates I=mapped/1_KPK1.bam O=marked/test.bam M=marked/test.txt
#WORKS
```

##### TRUE

```{python}
from os.path import join

# Globals                              

SW_DIR = "/nfs/software/birney/"
DATA_DIR = "/nfs/research1/birney/projects/medaka/deepseqlab/"
(LANE_1, LANE_2, SAMPLES, PAIRS) = glob_wildcards(join(DATA_DIR, "H2YM2BBXY_MC259_19s000097-{lane_1}-1_Ibberson_lane{lane_2,\d+}{sample}_{pair}_sequence.txt.gz"))
REF = "/nfs/research1/birney/users/brettell/ref/Oryzias_latipes.ASM223467v1.dna.toplevel.fa.gz"
BWA = join(SW_DIR, "bwa/bwa")
PICARD = join(SW_DIR, "picard.jar")

# Rules                                

rule all:
    input:
        expand("marked/{lane_1}_{sample}.{format}", sample = SAMPLES, lane_1 = LANE_1, format = ["bam", "txt"])

rule bwa_mem_1:
    input:
        l1_r1 = join(DATA_DIR, "H2YM2BBXY_MC259_19s000097-1-1_Ibberson_lane5{sample}_1_sequence.txt.gz"),
        l1_r2 = join(DATA_DIR, "H2YM2BBXY_MC259_19s000097-1-1_Ibberson_lane5{sample}_2_sequence.txt.gz")
    output:
        "mapped/1_{sample}.bam"
    shell:
        "{BWA} mem -M -t 16 {REF} {input.l1_r1} {input.l1_r2} > {output}"

rule bwa_mem_2:
    input:
        l2_r1 = join(DATA_DIR, "H2YM2BBXY_MC259_19s000097-2-1_Ibberson_lane6{sample}_1_sequence.txt.gz"),
        l2_r2 = join(DATA_DIR, "H2YM2BBXY_MC259_19s000097-2-1_Ibberson_lane6{sample}_2_sequence.txt.gz")
    output:
        "mapped/2_{sample}.bam"
    shell:
        "{BWA} mem -M -t 16 {REF} {input.l2_r1} {input.l2_r2} > {output}"

rule picard_sort_sam:
    input:
        "mapped/{lane_1}_{sample}.bam"
    output:
        bam = "sorted/{lane_1}_{sample}.bam",
        bai = "sorted/{lane_1}_{sample}.bai"
    shell:
        "java -Xmx10g -Djava.io.tmpdir=/tmp -jar {PICARD} SortSam SO=coordinate INPUT={input} OUTPUT={output.bam} VALIDATION_STRINGENCY=LENIENT CREATE_INDEX=true"

rule picard_mark_duplicates:
    input:
        "sorted/{lane_1}_{sample}.bam"
    output:
        bam = "marked/{lane_1}_{sample}.bam",
        metrics = "marked/{lane_1}_{sample}.txt"
    shell:
        "java -Xmx10g -jar {PICARD} MarkDuplicates I={input} O={output.bam} M={output.metrics}"

# SUCCESS!
```

### HaplotypeCaller

#### Tom's code
```{bash, eval = F}
BAMPATH="../bam"
REF="/homes/tomas/birney/projects/medaka/inbred_panel/medaka-alignments-release-94/ref/Oryzias_latipes.ASM223467v1.dna.toplevel.fa"
OUTPATH="../gvcf"
FILES=($(ls $BAMPATH | grep bam))
f=${FILES[$LSB_JOBINDEX-1]}
fbname=$(basename "$f" .bam)

if [ -f $OUTPATH/$fbname.g.vcf.idx ]; then
   echo "File $fbname.idx exists."
else
	GATK="java -jar /nfs/software/birney/GATK/GenomeAnalysisTK.jar -T HaplotypeCaller  "
	COMMAND="$GATK "
	COMMAND="$COMMAND -R $REF "
	COMMAND="$COMMAND -I $BAMPATH/$f "
	COMMAND="$COMMAND -o $OUTPATH/$fbname.g.vcf"
	COMMAND="$COMMAND --emitRefConfidence GVCF"
	COMMAND="$COMMAND --variant_index_type LINEAR"
	COMMAND="$COMMAND --variant_index_parameter 128000"

	eval "$COMMAND"
fi
```

#### TEST
```{bash, eval = F}
# note - in .bashrc: export PATH=$PATH:/nfs/software/birney/gatk-4.1.0.0/
gatk --java-options "-Xmx4g" HaplotypeCaller  \
  -R /nfs/research1/birney/users/brettell/ref/Oryzias_latipes.ASM223467v1.dna.toplevel.fa \
  -I /nfs/research1/birney/users/brettell/tailess/marked/1_KPK1.bam \
  -O /nfs/research1/birney/users/brettell/tailess/gvcfs/1_KPK1.g.vcf.gz \
  -ERC GVCF
# A USER ERROR has occurred: Argument --emit-ref-confidence has a bad value: Can only be used in single sample mode currently. Use the --sample-name argument to run on a single sample out of a multi-sample BAM file.

# try indexing
/nfs/software/birney/samtools-1.9/samtools index /nfs/research1/birney/users/brettell/tailess/marked/1_KPK1.bam nfs/research1/birney/users/brettell/tailess/marked/1_KPK1.bam.bai
#ERROR: samtools index: failed to create or write index "nfs/research1/birney/users/brettell/tailess/marked/1_KPK1.bam.bai"

# repeat SamSort with more memory, then try indexing the output bam
/nfs/software/birney/samtools-1.9/samtools index /nfs/research1/birney/users/brettell/tailess/sorted/1_STS1.bam nfs/research1/birney/users/brettell/tailess/sorted/1_STS1.bam.bai
# samtools index: failed to create or write index "nfs/research1/birney/users/brettell/tailess/sorted/1_STS1.bam.bai"

```

Try sorting with samtools
```{python, eval = F}
from os.path import join

# Globals                              

SW_DIR = "/nfs/software/birney/"
DATA_DIR = "/nfs/research1/birney/projects/medaka/deepseqlab/"
(LANE_1, LANE_2, SAMPLES, PAIRS) = glob_wildcards(join(DATA_DIR, "H2YM2BBXY_MC259_19s000097-{lane_1}-1_Ibberson_lane{lane_2,\d+}{sample}_{pair}_sequence.txt.gz"))
REF = "/nfs/research1/birney/users/brettell/ref/Oryzias_latipes.ASM223467v1.dna.toplevel.fa.gz"
BWA = join(SW_DIR, "bwa/bwa")
PICARD = join(SW_DIR, "picard.jar")

# Rules                                

rule all:
    input:
        expand("samtools_sorted/{lane_1}_{sample}.bam", sample = SAMPLES, lane_1 = LANE_1)

rule bwa_mem_1:
    input:
        l1_r1 = join(DATA_DIR, "H2YM2BBXY_MC259_19s000097-1-1_Ibberson_lane5{sample}_1_sequence.txt.gz"),
        l1_r2 = join(DATA_DIR, "H2YM2BBXY_MC259_19s000097-1-1_Ibberson_lane5{sample}_2_sequence.txt.gz")
    output:
        "mapped/1_{sample}.bam"
    shell:
        "{BWA} mem -M -t 16 {REF} {input.l1_r1} {input.l1_r2} > {output}"

rule bwa_mem_2:
    input:
        l2_r1 = join(DATA_DIR, "H2YM2BBXY_MC259_19s000097-2-1_Ibberson_lane6{sample}_1_sequence.txt.gz"),
        l2_r2 = join(DATA_DIR, "H2YM2BBXY_MC259_19s000097-2-1_Ibberson_lane6{sample}_2_sequence.txt.gz")
    output:
        "mapped/2_{sample}.bam"
    shell:
        "{BWA} mem -M -t 16 {REF} {input.l2_r1} {input.l2_r2} > {output}"

rule samtools_sort_sam:
    input:
        "mapped/{lane_1}_{sample}.bam"
    output:
        "samtools_sorted/{lane_1}_{sample}.bam"
    shell:
        "/nfs/software/birney/samtools-1.9/samtools sort -l 0 -o {output} -O bam -@ 8 {input}"

# SUCCESS: sizes range from 16G (STS2) to 48G (SOS2)   

```

Check stats
```{bash, eval = F}
samtools view samtools_sorted/1_KPK1.bam | wc -l
# 125,691,514

/nfs/software/birney/samtools-1.9/samtools stats samtools_sorted/1_KPK1.bam
#

/nfs/software/birney/samtools-1.9/samtools stats samtools_sorted/1_KPK2.bam
#

```


Try indexing
```{bash, eval = F}
bsub -M 10000 -Is bash
/nfs/software/birney/samtools-1.9/samtools index /nfs/research1/birney/users/brettell/tailess/samtools_sorted/1_KPK1.bam nfs/research1/birney/users/brettell/tailess/samtools_sorted/1_KPK1.bam.bai
# samtools index: failed to create or write index "nfs/research1/birney/users/brettell/tailess/samtools_sorted/1_KPK1.bam.bai"

java -Xmx50g -jar /nfs/software/birney/picard.jar BuildBamIndex I=/nfs/research1/birney/users/brettell/tailess/samtools_sorted/1_KPK1.bam
# SUCCESS! but...
```

Validate BAM files
```{bash, eval = F}
bsub -M 10000 -Is bash
java -Xmx50g -jar /nfs/software/birney/picard.jar ValidateSamFile I=samtools_sorted/1_KPK1.bam
#ERROR: Header version: 1.6 does not match any of the acceptable versions: 1.0, 1.3, 1.4, 1.5
#ERROR: Read groups is empty

# running on the mapped BAM:
java -Xmx50g -jar /nfs/software/birney/picard.jar ValidateSamFile I=mapped/1_KPK1.bam
# ERROR: Read groups is empty
```

### AddOrReplaceReadGroups

#### Tom's script
```{bash, eval = F}
ADDREPLACEREADGREOUPS="java -jar /nfs/software/birney/picard.jar AddOrReplaceReadGroups "

	IFS='# ' read -r -a name_items <<< "$f"
	IFS='_ ' read -r -a items <<< "${name_items[0]}"

	COMMAND="$ADDREPLACEREADGREOUPS I=$BAMPATH/$fbname.bam "
	COMMAND="$COMMAND O=$BAMPATH/$fbname.t.bam "
	COMMAND="$COMMAND RGID=1 "
	COMMAND="$COMMAND RGLB=DN474009U "
	COMMAND="$COMMAND RGPL=illumina "
	COMMAND="$COMMAND RGPU=${items[0]} "
	COMMAND="$COMMAND RGSM=$fbname"

	eval "$COMMAND"
```


#### TEST
```{bash, eval = F}
mkdir added_read_groups

java -jar /nfs/software/birney/picard.jar AddOrReplaceReadGroups \
  I=mapped/1_KPK1.bam \
  O=added_read_groups/1_KPK1.bam \
  RGID=1 \
  RGLB=tailless \
  RGPL=illumina \
  RGPU=KPK1 \
  RGSM=1_KPK1

# WORKS
```

#### TRUE
```{python, eval = F}
from os.path import join

# Globals                              

SW_DIR = "/nfs/software/birney/"
DATA_DIR = "/nfs/research1/birney/projects/medaka/deepseqlab/"
(LANE_1, LANE_2, SAMPLES, PAIRS) = glob_wildcards(join(DATA_DIR, "H2YM2BBXY_MC259_19s000097-{lane_1}-1_Ibberson_lane{lane_2,\d+}{sample}_{pair}_sequence.txt.gz"))
REF = "/nfs/research1/birney/users/brettell/ref/Oryzias_latipes.ASM223467v1.dna.toplevel.fa.gz"
BWA = join(SW_DIR, "bwa/bwa")
PICARD = join(SW_DIR, "picard-2.9.0/picard.jar")

# Rules                                

rule all:
    input:
        expand("sorted/{lane_1}_{sample}.{format}", sample = SAMPLES, lane_1 = LANE_1, format = ["bam", "bai"])

rule bwa_mem_1:
    input:
        l1_r1 = join(DATA_DIR, "H2YM2BBXY_MC259_19s000097-1-1_Ibberson_lane5{sample}_1_sequence.txt.gz"),
        l1_r2 = join(DATA_DIR, "H2YM2BBXY_MC259_19s000097-1-1_Ibberson_lane5{sample}_2_sequence.txt.gz")
    output:
        "mapped/1_{sample}.bam"
    shell:
        "{BWA} mem -M -t 16 {REF} {input.l1_r1} {input.l1_r2} > {output}"

rule bwa_mem_2:
    input:
        l2_r1 = join(DATA_DIR, "H2YM2BBXY_MC259_19s000097-2-1_Ibberson_lane6{sample}_1_sequence.txt.gz"),
        l2_r2 = join(DATA_DIR, "H2YM2BBXY_MC259_19s000097-2-1_Ibberson_lane6{sample}_2_sequence.txt.gz")
    output:
        "mapped/2_{sample}.bam"
    shell:
        "{BWA} mem -M -t 16 {REF} {input.l2_r1} {input.l2_r2} > {output}"

rule picard_add_read_groups:
    input:
        "mapped/{lane_1}_{sample}.bam"
    params:
        sample = "{sample}",
        lane = "{lane_1}"
    output:
        "added_read_groups/{lane_1}_{sample}.bam"
    shell:
        "java -Xmx50g -jar {PICARD} AddOrReplaceReadGroups I={input} O={output} RGID=1 RGLB=tailless RGPL=illumina RGPU={params.sample} RGSM={params.sample}_{params.lane} TMP_DIR=/nfs/research1/birney/users/brettell/tailess/added_read_groups"

rule picard_sort_sam:
    input:
        "added_read_groups/{lane_1}_{sample}.bam"
    output:
        bam = "sorted/{lane_1}_{sample}.bam",
        bai = "sorted/{lane_1}_{sample}.bai"
    shell:
        "java -Xmx50g -jar {PICARD} SortSam SO=coordinate INPUT={input} OUTPUT={output.bam} VALIDATION_STRINGENCY=LENIENT CREATE_INDEX=true TMP_DIR=/nfs/research1/birney/users/brettell/tailess/sorted"

```

Quality control
```{bash, eval = F}
# count lines in mapped bam (19G)
/nfs/software/birney/samtools-1.9/samtools view mapped/1_STS2.bam | wc -l
#48332619

# count lines in bam with read groups added (5.2G)
/nfs/software/birney/samtools-1.9/samtools view added_read_groups/1_STS2.bam | wc -l
#48332619
# exactly the same!

# Validate added_read_groups/BAM
java -Xmx50g -jar /nfs/software/birney/picard.jar ValidateSamFile I=added_read_groups/1_STS2.bam
# No errors found

# Validate sorted/BAM
java -Xmx50g -jar /nfs/software/birney/picard.jar ValidateSamFile I=sorted/1_STS2.bam
# No errors found

# count lines in sorted/BAM
/nfs/software/birney/samtools-1.9/samtools view sorted/1_STS2.bam | wc -l
#48332619
# still the same!

# there are two samples that have different sizes in added_read_groups: 1_SOS2 and 2_SOS2 (16 and 16G). Check number of lines
/nfs/software/birney/samtools-1.9/samtools view added_read_groups/1_SOS2.bam | wc -l
#152863154
/nfs/software/birney/samtools-1.9/samtools view added_read_groups/2_SOS2.bam | wc -l
#151869635

# is this the same for mapped?
/nfs/software/birney/samtools-1.9/samtools view mapped/1_SOS2.bam | wc -l
# 152863154
/nfs/software/birney/samtools-1.9/samtools view mapped/2_SOS2.bam | wc -l
# 151869635
```

### MarkDuplicates
```{python}
from os.path import join

# Globals                              

SW_DIR = "/nfs/software/birney/"
DATA_DIR = "/nfs/research1/birney/projects/medaka/deepseqlab/"
(LANE_1, LANE_2, SAMPLES, PAIRS) = glob_wildcards(join(DATA_DIR, "H2YM2BBXY_MC259_19s000097-{lane_1}-1_Ibberson_lane{lane_2,\d+}{sample}_{pair}_sequence.txt.gz"))
REF = "/nfs/research1/birney/users/brettell/ref/Oryzias_latipes.ASM223467v1.dna.toplevel.fa.gz"
BWA = join(SW_DIR, "bwa/bwa")
PICARD = join(SW_DIR, "picard-2.9.0/picard.jar")

# Rules                                

rule all:
    input:
        expand("marked/{lane_1}_{sample}.{format}", sample = SAMPLES, lane_1 = LANE_1, format = ["bam", "txt"])

rule bwa_mem_1:
    input:
        l1_r1 = join(DATA_DIR, "H2YM2BBXY_MC259_19s000097-1-1_Ibberson_lane5{sample}_1_sequence.txt.gz"),
        l1_r2 = join(DATA_DIR, "H2YM2BBXY_MC259_19s000097-1-1_Ibberson_lane5{sample}_2_sequence.txt.gz")
    output:
        "mapped/1_{sample}.bam"
    shell:
        "{BWA} mem -M -t 16 {REF} {input.l1_r1} {input.l1_r2} > {output}"

rule bwa_mem_2:
    input:
        l2_r1 = join(DATA_DIR, "H2YM2BBXY_MC259_19s000097-2-1_Ibberson_lane6{sample}_1_sequence.txt.gz"),
        l2_r2 = join(DATA_DIR, "H2YM2BBXY_MC259_19s000097-2-1_Ibberson_lane6{sample}_2_sequence.txt.gz")
    output:
        "mapped/2_{sample}.bam"
    shell:
        "{BWA} mem -M -t 16 {REF} {input.l2_r1} {input.l2_r2} > {output}"

rule picard_add_read_groups:
    input:
        "mapped/{lane_1}_{sample}.bam"
    params:
        sample = "{sample}",
        lane = "{lane_1}"
    output:
        "added_read_groups/{lane_1}_{sample}.bam"
    shell:
        "java -Xmx50g -jar {PICARD} AddOrReplaceReadGroups I={input} O={output} RGID=1 RGLB=tailless RGPL=illumina RGPU={params.sample} RGSM={params.sample}_{params.lane} TMP_DIR=/nfs/research1/birney/users/brettell/tailess/added_read_groups"

rule picard_sort_sam:
    input:
        "added_read_groups/{lane_1}_{sample}.bam"
    output:
        bam = "sorted/{lane_1}_{sample}.bam",
        bai = "sorted/{lane_1}_{sample}.bai"
    shell:
        "java -Xmx50g -jar {PICARD} SortSam SO=coordinate INPUT={input} OUTPUT={output.bam} VALIDATION_STRINGENCY=LENIENT CREATE_INDEX=true TMP_DIR=/nfs/research1/birney/users/brettell/tailess/sorted"

rule picard_mark_duplicates:
    input:
        "sorted/{lane_1}_{sample}.bam"
    output:
        bam = "marked/{lane_1}_{sample}.bam",
        metrics = "marked/{lane_1}_{sample}.txt"
    shell:
        "java -Xmx50g -jar {PICARD} MarkDuplicates I={input} O={output.bam} M={output.metrics}"

# SUCCESS?        
```

Validate
```{bash, eval = F}
bsub -M 10000 -Is bash
java -Xmx50g -jar /nfs/software/birney/picard.jar ValidateSamFile I=marked/1_KPK1.bam
# No errors found
```

### Create Indexes
```{python, eval = F}
from os.path import join

# Globals                              

SW_DIR = "/nfs/software/birney/"
DATA_DIR = "/nfs/research1/birney/projects/medaka/deepseqlab/"
(LANE_1, LANE_2, SAMPLES, PAIRS) = glob_wildcards(join(DATA_DIR, "H2YM2BBXY_MC259_19s000097-{lane_1}-1_Ibberson_lane{lane_2,\d+}{sample}_{pair}_sequence.txt.gz"))
REF = "/nfs/research1/birney/users/brettell/ref/Oryzias_latipes.ASM223467v1.dna.toplevel.fa.gz"
BWA = join(SW_DIR, "bwa/bwa")
PICARD = join(SW_DIR, "picard-2.9.0/picard.jar")

# Rules                                

rule all:
    input:
        expand("marked/{lane_1}_{sample}.bai", sample = SAMPLES, lane_1 = LANE_1)

rule bwa_mem_1:
    input:
        l1_r1 = join(DATA_DIR, "H2YM2BBXY_MC259_19s000097-1-1_Ibberson_lane5{sample}_1_sequence.txt.gz"),
        l1_r2 = join(DATA_DIR, "H2YM2BBXY_MC259_19s000097-1-1_Ibberson_lane5{sample}_2_sequence.txt.gz")
    output:
        "mapped/1_{sample}.bam"
    shell:
        "{BWA} mem -M -t 16 {REF} {input.l1_r1} {input.l1_r2} > {output}"

rule bwa_mem_2:
    input:
        l2_r1 = join(DATA_DIR, "H2YM2BBXY_MC259_19s000097-2-1_Ibberson_lane6{sample}_1_sequence.txt.gz"),
        l2_r2 = join(DATA_DIR, "H2YM2BBXY_MC259_19s000097-2-1_Ibberson_lane6{sample}_2_sequence.txt.gz")
    output:
        "mapped/2_{sample}.bam"
    shell:
        "{BWA} mem -M -t 16 {REF} {input.l2_r1} {input.l2_r2} > {output}"

rule picard_add_read_groups:
    input:
        "mapped/{lane_1}_{sample}.bam"
    params:
        sample = "{sample}",
        lane = "{lane_1}"
    output:
        "added_read_groups/{lane_1}_{sample}.bam"
    shell:
        "java -Xmx50g -jar {PICARD} AddOrReplaceReadGroups I={input} O={output} RGID=1 RGLB=tailless RGPL=illumina RGPU={params.sample} RGSM={params.sample}_{params.lane} TMP_DIR=/nfs/research1/birney/users/brettell/tailess/added_read_groups"

rule picard_sort_sam:
    input:
        "added_read_groups/{lane_1}_{sample}.bam"
    output:
        bam = "sorted/{lane_1}_{sample}.bam",
        bai = "sorted/{lane_1}_{sample}.bai"
    shell:
        "java -Xmx50g -jar {PICARD} SortSam SO=coordinate INPUT={input} OUTPUT={output.bam} VALIDATION_STRINGENCY=LENIENT CREATE_INDEX=true TMP_DIR=/nfs/research1/birney/users/brettell/tailess/sorted"

rule picard_mark_duplicates:
    input:
        "sorted/{lane_1}_{sample}.bam"
    output:
        bam = "marked/{lane_1}_{sample}.bam",
        metrics = "marked/{lane_1}_{sample}.txt"
    shell:
        "java -Xmx50g -jar {PICARD} MarkDuplicates I={input} O={output.bam} M={output.metrics}"

rule picard_build_bam_index:
    input:
        "marked/{lane_1}_{sample}.bam"
    output:
        "marked/{lane_1}_{sample}.bai"
    shell:
        "java -Xmx50g -jar {PICARD} BuildBamIndex I={input}"

# SUCCESS        
```

### HaplotypeCaller

#### TEST
```{bash, eval = F}
GATK = join(SW_DIR, "gatk-4.1.0.0/gatk")
/nfs/software/birney/gatk-4.1.0.0/gatk --java-options -Xmx50G HaplotypeCaller \
  -R /nfs/research1/birney/users/brettell/ref/Oryzias_latipes.ASM223467v1.dna.toplevel.fa \
  -I /nfs/research1/birney/users/brettell/tailess/marked/1_KPK1.bam \
  -O /nfs/research1/birney/users/brettell/tailess/gvcfs/1_KPK1.g.vcf.gz \
  -ERC GVCF

# seems to work  
```

#### TRUE
```{python, eval = F}
from os.path import join

# Globals                              

SW_DIR = "/nfs/software/birney/"
DATA_DIR = "/nfs/research1/birney/projects/medaka/deepseqlab/"
(LANE_1, LANE_2, SAMPLES, PAIRS) = glob_wildcards(join(DATA_DIR, "H2YM2BBXY_MC259_19s000097-{lane_1}-1_Ibberson_lane{lane_2,\d+}{sample}_{pair}_sequence.txt.gz"))
REF_ZIP = "/nfs/research1/birney/users/brettell/ref/Oryzias_latipes.ASM223467v1.dna.toplevel.fa.gz"
REF_UNZIP = "/nfs/research1/birney/users/brettell/ref/Oryzias_latipes.ASM223467v1.dna.toplevel.fa"
BWA = join(SW_DIR, "bwa/bwa")
PICARD = join(SW_DIR, "picard-2.9.0/picard.jar")
GATK = join(SW_DIR, "gatk-4.1.0.0/gatk")

# Rules                                

rule all:
    input:
        expand("gvcfs/{lane_1}_{sample}.g.vcf.gz", sample = SAMPLES, lane_1 = LANE_1)

rule bwa_mem_1:
    input:
        l1_r1 = join(DATA_DIR, "H2YM2BBXY_MC259_19s000097-1-1_Ibberson_lane5{sample}_1_sequence.txt.gz"),
        l1_r2 = join(DATA_DIR, "H2YM2BBXY_MC259_19s000097-1-1_Ibberson_lane5{sample}_2_sequence.txt.gz")
    output:
        "mapped/1_{sample}.bam"
    shell:
        "{BWA} mem -M -t 16 {REF_ZIP} {input.l1_r1} {input.l1_r2} > {output}"

rule bwa_mem_2:
    input:
        l2_r1 = join(DATA_DIR, "H2YM2BBXY_MC259_19s000097-2-1_Ibberson_lane6{sample}_1_sequence.txt.gz"),
        l2_r2 = join(DATA_DIR, "H2YM2BBXY_MC259_19s000097-2-1_Ibberson_lane6{sample}_2_sequence.txt.gz")
    output:
        "mapped/2_{sample}.bam"
    shell:
        "{BWA} mem -M -t 16 {REF_ZIP} {input.l2_r1} {input.l2_r2} > {output}"

rule picard_add_read_groups:
    input:
        "mapped/{lane_1}_{sample}.bam"
    params:
        sample = "{sample}",
        lane = "{lane_1}"
    output:
        "added_read_groups/{lane_1}_{sample}.bam"
    shell:
        "java -Xmx50g -jar {PICARD} AddOrReplaceReadGroups I={input} O={output} RGID=1 RGLB=tailless RGPL=illumina RGPU={params.sample} RGSM={params.sample}_{params.lane} TMP_DIR=/nfs/research1/birney/users/brettell/tailess/added_read_groups"

rule picard_sort_sam:
    input:
        "added_read_groups/{lane_1}_{sample}.bam"
    output:
        bam = "sorted/{lane_1}_{sample}.bam",
        bai = "sorted/{lane_1}_{sample}.bai"
    shell:
        "java -Xmx50g -jar {PICARD} SortSam SO=coordinate INPUT={input} OUTPUT={output.bam} VALIDATION_STRINGENCY=LENIENT CREATE_INDEX=true TMP_DIR=/nfs/research1/birney/users/brettell/tailess/sorted"

rule picard_mark_duplicates:
    input:
        "sorted/{lane_1}_{sample}.bam"
    output:
        bam = "marked/{lane_1}_{sample}.bam",
        metrics = "marked/{lane_1}_{sample}.txt"
    shell:
        "java -Xmx50g -jar {PICARD} MarkDuplicates I={input} O={output.bam} M={output.metrics}"

rule picard_build_bam_index:
    input:
        "marked/{lane_1}_{sample}.bam"
    output:
        "marked/{lane_1}_{sample}.bai"
    shell:
        "java -Xmx50g -jar {PICARD} BuildBamIndex I={input}"

rule gatk_haplotype_caller:
    input:
        "marked/{lane_1}_{sample}.bam"
    output:
        "gvcfs/{lane_1}_{sample}.g.vcf.gz"
    shell:
        "{GATK} --java-options \"-Xmx50G\" HaplotypeCaller -R {REF_UNZIP} -I {input} -O {output} -ERC GVCF"

# After 9 hours it didn't even finish chromosome 1. Try runnning on each chr individually.        
```

```{python, eval = F}
from os.path import join

# Globals                              

SW_DIR = "/nfs/software/birney/"
DATA_DIR = "/nfs/research1/birney/projects/medaka/deepseqlab/"
(LANE_1, LANE_2, SAMPLES, PAIRS) = glob_wildcards(join(DATA_DIR, "H2YM2BBXY_MC259_19s000097-{lane_1}-1_Ibberson_lane{lane_2,\d+}{sample}_{pair}_sequence.txt.gz"))
REF_ZIP = "/nfs/research1/birney/users/brettell/ref/Oryzias_latipes.ASM223467v1.dna.toplevel.fa.gz"
REF_UNZIP = "/nfs/research1/birney/users/brettell/ref/Oryzias_latipes.ASM223467v1.dna.toplevel.fa"
CHR = ["1", "2", "3", "4", "5", "6", "7", "8", "9", "10", "11", "12", "13", "14", "15", "16", "17", "18", "19", "20", "21", "22", "23", "24", "MT"]
BWA = join(SW_DIR, "bwa/bwa")
PICARD = join(SW_DIR, "picard-2.9.0/picard.jar")
GATK = join(SW_DIR, "gatk-4.1.0.0/gatk")

# Rules                                

rule all:
    input:
        expand("gvcfs/{lane_1}_{sample}/{chr}.g.vcf.gz", lane_1 = LANE_1, sample = SAMPLES, chr = CHR)

rule bwa_mem_1:
    input:
        l1_r1 = join(DATA_DIR, "H2YM2BBXY_MC259_19s000097-1-1_Ibberson_lane5{sample}_1_sequence.txt.gz"),
        l1_r2 = join(DATA_DIR, "H2YM2BBXY_MC259_19s000097-1-1_Ibberson_lane5{sample}_2_sequence.txt.gz")
    output:
        "mapped/1_{sample}.bam"
    shell:
        "{BWA} mem -M -t 16 {REF_ZIP} {input.l1_r1} {input.l1_r2} > {output}"

rule bwa_mem_2:
    input:
        l2_r1 = join(DATA_DIR, "H2YM2BBXY_MC259_19s000097-2-1_Ibberson_lane6{sample}_1_sequence.txt.gz"),
        l2_r2 = join(DATA_DIR, "H2YM2BBXY_MC259_19s000097-2-1_Ibberson_lane6{sample}_2_sequence.txt.gz")
    output:
        "mapped/2_{sample}.bam"
    shell:
        "{BWA} mem -M -t 16 {REF_ZIP} {input.l2_r1} {input.l2_r2} > {output}"

rule picard_add_read_groups:
    input:
        "mapped/{lane_1}_{sample}.bam"
    params:
        sample = "{sample}",
        lane = "{lane_1}"
    output:
        "added_read_groups/{lane_1}_{sample}.bam"
    shell:
        "java -Xmx50g -jar {PICARD} AddOrReplaceReadGroups I={input} O={output} RGID=1 RGLB=tailless RGPL=illumina RGPU={params.sample} RGSM={params.sample}_{params.lane} TMP_DIR=/nfs/research1/birney/users/brettell/tailess/added_read_groups"

rule picard_sort_sam:
    input:
        "added_read_groups/{lane_1}_{sample}.bam"
    output:
        bam = "sorted/{lane_1}_{sample}.bam",
        bai = "sorted/{lane_1}_{sample}.bai"
    shell:
        "java -Xmx50g -jar {PICARD} SortSam SO=coordinate INPUT={input} OUTPUT={output.bam} VALIDATION_STRINGENCY=LENIENT CREATE_INDEX=true TMP_DIR=/nfs/research1/birney/users/brettell/tailess/sorted"

rule picard_mark_duplicates:
    input:
        "sorted/{lane_1}_{sample}.bam"
    output:
        bam = "marked/{lane_1}_{sample}.bam",
        metrics = "marked/{lane_1}_{sample}.txt"
    shell:
        "java -Xmx50g -jar {PICARD} MarkDuplicates I={input} O={output.bam} M={output.metrics}"

rule picard_build_bam_index:
    input:
        "marked/{lane_1}_{sample}.bam"
    output:
        "marked/{lane_1}_{sample}.bai"
    shell:
        "java -Xmx50g -jar {PICARD} BuildBamIndex I={input}"

rule gatk_haplotype_caller:
    input:
        "marked/{lane_1}_{sample}.bam"
    params:
        chr = "{chr}"
    output:
        "gvcfs/{lane_1}_{sample}/{chr}.g.vcf.gz" # note: this can't use "{params.chr}" - needs to be {chr} from the global
    shell:
        "{GATK} --java-options \"-Xmx50G\" HaplotypeCaller -R {REF_UNZIP} -I {input} -L {params.chr} -O {output} -ERC GVCF"


```

This one is still running after nearly 24 hours (job ID 7861920): `/nfs/software/birney/gatk-4.1.0.0/gatk --java-options "-Xmx50G" HaplotypeCaller -R /nfs/research1/birney/users/brettell/ref/Oryzias_latipes.ASM223467v1.dna.toplevel.fa -I marked/2_KPK2.bam -L 3 -O gvcfs/2_KPK2/3.g.vcf.gz -ERC GVCF`. Run again:
```{bash, eval = F}
bsub -M 70000 -o log/rerun_gvcfs_2_KPK2.out -e log/rerun_gvcfs_2_KPK2.err /nfs/software/birney/gatk-4.1.0.0/gatk --java-options "-Xmx70G" HaplotypeCalle
r -R /nfs/research1/birney/users/brettell/ref/Oryzias_latipes.ASM223467v1.dna.toplevel.fa -I marked/2_KPK2.bam -L 3 -O gvcfs/2_KPK2/3.g.vcf.gz -ERC GVCF
#Job <9573320> is submitted to default queue <research-rh7>.
```


### Collate VCFs

#### Per sample
```{python, eval = F}

from os.path import join

# Globals                              

DATA_DIR = "/nfs/research1/birney/projects/medaka/deepseqlab/"
LANES = ["H2YM2BBXY_MC259_19s000097-1-1_Ibberson_lane5", "H2YM2BBXY_MC259_19s000097-2-1_Ibberson_lane6"]
SAMPLES = ["KPK", "SOS", "STS"]
REPLS = ["1", "2"]
PAIRS = ["1", "2"]
REF_ZIP = "/nfs/research1/birney/users/brettell/ref/Oryzias_latipes.ASM223467v1.dna.toplevel.fa.gz"
SW_DIR = "/nfs/software/birney/"
BWA = join(SW_DIR, "bwa/bwa") # Version: 1.4-6-g5e49659 (using htslib 1.4-5-g10bc1a7)

rule all:
    input:
        expand("test/{lane}_{sample}{repl}.bam", lane = LANES, sample = SAMPLES, repl = REPLS)

def test_inputs(wildcards):
    files = expand(join(DATA_DIR, "{lane}{sample}{repl}_{pair}_sequence.txt.gz"), lane = wildcards.lane, sample = wildcards.sample, repl = wildcards.repl, pair = PAIRS)
    return files

rule test:
    input:
        test_inputs
    output:
        "test/{lane}_{sample}{repl}.bam"
    shell:
        "{BWA} mem -M -t 16 {REF_ZIP} {input} > {output}"

# WORKS. Tidier solution:      

from os.path import join

# Globals                              

DATA_DIR = "/nfs/research1/birney/projects/medaka/deepseqlab/"
LANES = ["H2YM2BBXY_MC259_19s000097-1-1_Ibberson_lane5", "H2YM2BBXY_MC259_19s000097-2-1_Ibberson_lane6"]
SAMPLES = ["KPK", "SOS", "STS"]
REPLS = ["1", "2"]
PAIRS = ["1", "2"]
REF_ZIP = "/nfs/research1/birney/users/brettell/ref/Oryzias_latipes.ASM223467v1.dna.toplevel.fa.gz"
SW_DIR = "/nfs/software/birney/"
BWA = join(SW_DIR, "bwa/bwa") # Version: 1.4-6-g5e49659 (using htslib 1.4-5-g10bc1a7)

rule all:
    input:
        expand("test/{lane}{sample}{repl}.bam", lane = LANES, sample = SAMPLES, repl = REPLS)

rule test:
    input:
        expand(join(DATA_DIR, "{{lane}}{{sample}}{{repl}}_{pair}_sequence.txt.gz"), pair = PAIRS)
    output:
        "test/{lane}{sample}{repl}.bam"
    shell:
        "{BWA} mem -M -t 16 {REF_ZIP} {input} > {output}"

##############        
# Now to replace the long LANES strings with "1" or "2"

from os.path import join

# Globals                              

DATA_DIR = "/nfs/research1/birney/projects/medaka/deepseqlab/"
LANES = ["H2YM2BBXY_MC259_19s000097-1-1_Ibberson_lane5", "H2YM2BBXY_MC259_19s000097-2-1_Ibberson_lane6"]
SAMPLES = ["KPK", "SOS", "STS"]
REPLS = ["1", "2"]
PAIRS = ["1", "2"]
REF_ZIP = "/nfs/research1/birney/users/brettell/ref/Oryzias_latipes.ASM223467v1.dna.toplevel.fa.gz"
SW_DIR = "/nfs/software/birney/"
BWA = join(SW_DIR, "bwa/bwa") # Version: 1.4-6-g5e49659 (using htslib 1.4-5-g10bc1a7)

rule all:
    input:
        expand("test/{lane_shrt}_{sample}{repl}.bam", lane_shrt = ["1", "2"], sample = SAMPLES, repl = REPLS)

rule test:
    input:
        expand(join(DATA_DIR, "{{lane}}{{sample}}{{repl}}_{pair}_sequence.txt.gz"), lane = LANES, pair = PAIRS)
    output:
        "test/{lane}{sample}{repl}.bam"
    shell:
        "{BWA} mem -M -t 16 {REF_ZIP} {input} > {output}"

rule rename:
    output:
        mvto = "{sample}{repl}.bam"
    run:
        if wildcards.lane == "H2YM2BBXY_MC259_19s000097-1-1_Ibberson_lane5":
            mvfrom = "test/{lane}{sample}{repl}.bam"
            shell("mv {mvfrom} test/1_{output.mvto}")
        else:
            mvfrom = "test/{lane}{sample}{repl}.bam"
            shell("mv {mvfrom} test/2_{output.mvto}")

# ^Seems to work maybe?
```

```{bash, eval = F}
home
cd tailess
mkcd test

# copy half of the sorted files (smaller size than mapped) to test the code
for i in $(ls sorted/*1_sequence.txt.gz); do cp $i test; done


```

```{python, eval = F}
from os.path import join

# Globals                              

DATA_DIR = "/nfs/research1/birney/projects/medaka/deepseqlab/"
LANES = ["H2YM2BBXY_MC259_19s000097-1-1_Ibberson_lane5", "H2YM2BBXY_MC259_19s000097-2-1_Ibberson_lane6"]
SAMPLES = ["KPK", "SOS", "STS"]
REPLS = ["1", "2"]
PAIRS = ["1", "2"]
REF_ZIP = "/nfs/research1/birney/users/brettell/ref/Oryzias_latipes.ASM223467v1.dna.toplevel.fa.gz"
SW_DIR = "/nfs/software/birney/"
BWA = join(SW_DIR, "bwa/bwa") # Version: 1.4-6-g5e49659 (using htslib 1.4-5-g10bc1a7)

rule all:
    input:
        expand("test/name_changed/{lane_shrt}_{sample}{repl}.bam", lane_shrt = ["1", "2"], sample = SAMPLES, repl = REPLS)

rule rename:
    input:
        "test/{lane}{sample}{repl}_1_sequence.txt.gz"
    output:
        name = "test/name_changed/1_{sample}{repl}.txt.gz" if config["lane.lane1"] else "test/name_changed/2_{sample}{repl}.txt.gz"
    shell:
        "mv {input} {output.name}"
```

```{bash}
{
    "__default__" :
    {
        "memory" : "50000",
        "n" : "1",
        "resources" : "\"select[mem>50000] rusage[mem=50000] span[hosts=1]\"",
        "queue" : "research-rh7",
        "name" : "{rule}.{wildcards}",
        "output" : "log/{rule}_{wildcards}.out",
        "error" : "log/{rule}_{wildcards}.err"
    },
    "rename" :
    {
        "n" : "16",
        "lane" :
        {
            "lane_1" : "H2YM2BBXY_MC259_19s000097-1-1_Ibberson_lane5",
            "lane_2" : "H2YM2BBXY_MC259_19s000097-1-1_Ibberson_lane6"
        }    
    }
}
```

```{python}

############################################
from os.path import join

# Globals                              

SW_DIR = "/nfs/software/birney/"
DATA_DIR = "/nfs/research1/birney/projects/medaka/deepseqlab/"
(LANE_1, LANE_2, SAMPLES, PAIRS) = glob_wildcards(join(DATA_DIR, "H2YM2BBXY_MC259_19s000097-{lane_1}-1_Ibberson_lane{lane_2,\d+}{sample}_{pair}_sequence.txt.gz"))
REF_ZIP = "/nfs/research1/birney/users/brettell/ref/Oryzias_latipes.ASM223467v1.dna.toplevel.fa.gz"
REF_UNZIP = "/nfs/research1/birney/users/brettell/ref/Oryzias_latipes.ASM223467v1.dna.toplevel.fa"
CHR = ["1", "2", "3", "4", "5", "6", "7", "8", "9", "10", "11", "12", "13", "14", "15", "16", "17", "18", "19", "20", "21", "22", "23", "24", "MT"]
BWA = join(SW_DIR, "bwa/bwa")
PICARD = join(SW_DIR, "picard-2.9.0/picard.jar")
GATK = join(SW_DIR, "gatk-4.1.0.0/gatk")
BCF = join(SW_DIR, "bcftools/bcftools") # Version: 1.4-6-g5e49659 (using htslib 1.4-5-g10bc1a7)

# Rules                                

rule all:
    input:
        expand("gvcfs/gathered/{lane_1}_{sample}.g.vcf.gz", lane_1 = LANE_1, sample = SAMPLES)

rule bwa_mem_1:
    input:
        l1_r1 = join(DATA_DIR, "H2YM2BBXY_MC259_19s000097-1-1_Ibberson_lane5{sample}_1_sequence.txt.gz"),
        l1_r2 = join(DATA_DIR, "H2YM2BBXY_MC259_19s000097-1-1_Ibberson_lane5{sample}_2_sequence.txt.gz")
    output:
        "mapped/1_{sample}.bam"
    shell:
        "{BWA} mem -M -t 16 {REF_ZIP} {input.l1_r1} {input.l1_r2} > {output}"

rule bwa_mem_2:
    input:
        l2_r1 = join(DATA_DIR, "H2YM2BBXY_MC259_19s000097-2-1_Ibberson_lane6{sample}_1_sequence.txt.gz"),
        l2_r2 = join(DATA_DIR, "H2YM2BBXY_MC259_19s000097-2-1_Ibberson_lane6{sample}_2_sequence.txt.gz")
    output:
        "mapped/2_{sample}.bam"
    shell:
        "{BWA} mem -M -t 16 {REF_ZIP} {input.l2_r1} {input.l2_r2} > {output}"

rule picard_add_read_groups:
    input:
        "mapped/{lane_1}_{sample}.bam"
    params:
        sample = "{sample}",
        lane = "{lane_1}"
    output:
        "added_read_groups/{lane_1}_{sample}.bam"
    shell:
        "java -Xmx50g -jar {PICARD} AddOrReplaceReadGroups I={input} O={output} RGID=1 RGLB=tailless RGPL=illumina RGPU={params.sample} RGSM={params.sample}_{params.lane} TMP_DIR=/nfs/research1/birney/users/brettell/tailess/added_read_groups"

rule picard_sort_sam:
    input:
        "added_read_groups/{lane_1}_{sample}.bam"
    output:
        bam = "sorted/{lane_1}_{sample}.bam",
        bai = "sorted/{lane_1}_{sample}.bai"
    shell:
        "java -Xmx50g -jar {PICARD} SortSam SO=coordinate INPUT={input} OUTPUT={output.bam} VALIDATION_STRINGENCY=LENIENT CREATE_INDEX=true TMP_DIR=/nfs/research1/birney/users/brettell/tailess/sorted"

rule picard_mark_duplicates:
    input:
        "sorted/{lane_1}_{sample}.bam"
    output:
        bam = "marked/{lane_1}_{sample}.bam",
        metrics = "marked/{lane_1}_{sample}.txt"
    shell:
        "java -Xmx50g -jar {PICARD} MarkDuplicates I={input} O={output.bam} M={output.metrics}"

rule picard_build_bam_index:
    input:
        "marked/{lane_1}_{sample}.bam"
    output:
        "marked/{lane_1}_{sample}.bai"
    shell:
        "java -Xmx50g -jar {PICARD} BuildBamIndex I={input}"

rule gatk_haplotype_caller:
    input:
        "marked/{lane_1}_{sample}.bam"
    params:
        chr = "{chr}"
    output:
        "gvcfs/{lane_1}_{sample}/{chr}.g.vcf.gz" # note: this can't use "{params.chr}" - needs to be {chr} from the global
    shell:
        "{GATK} --java-options \"-Xmx50G\" HaplotypeCaller -R {REF_UNZIP} -I {input} -L {params.chr} -O {output} -ERC GVCF"

rule bcftools_combine_vcfs:
    input:
        expand("gvcfs/{{lane_1}}_{{sample}}/{chr}.g.vcf.gz", chr=CHR)
    output:
        "gvcfs/gathered/{lane_1}_{sample}.g.vcf.gz"
    shell:
        "java -Xmx50g -jar {PICARD} GatherVcfs {params.input_files} O={output}"


```

Picard seems to require the "I=" argument before every input file name. I can't figure out how to iterate this on Snakemake. Best to try using BCFTOOLS instead.

#### TEST
```{bash}
/nfs/software/birney/bcftools/bcftools concat gvcfs/2_STS1/1.g.vcf.gz gvcfs/2_STS1/2.g.vcf.gz gvcfs/2_STS1/3.g.vcf.gz gvcfs/2_STS1/4.g.vcf.gz gvcfs/2_STS1/5.g.vcf.gz gvcfs/2_STS1/6.g.vcf.gz gvcfs/2_STS1/7.g.vcf.gz gvcfs/2_STS1/8.g.vcf.gz gvcfs/2_STS1/9.g.vcf.gz gvcfs/2_STS1/10.g.vcf.gz gvcfs/2_STS1/11.g.vcf.gz gvcfs/2_STS1/12.g.vcf.gz gvcfs/2_STS1/13.g.vcf.gz gvcfs/2_STS1/14.g.vcf.gz gvcfs/2_STS1/15.g.vcf.gz gvcfs/2_STS1/16.g.vcf.gz gvcfs/2_STS1/17.g.vcf.gz gvcfs/2_STS1/18.g.vcf.gz gvcfs/2_STS1/19.g.vcf.gz gvcfs/2_STS1/20.g.vcf.gz gvcfs/2_STS1/21.g.vcf.gz gvcfs/2_STS1/22.g.vcf.gz gvcfs/2_STS1/23.g.vcf.gz gvcfs/2_STS1/24.g.vcf.gz gvcfs/2_STS1/MT.g.vcf.gz -o gvcfs/gathered/test.g.vcf.gz

# WORKS, except the "gz" file isn't zipped.
rm gvcfs/gathered/test.g.vcf.gz
```

#### TRUE
```{python}
from os.path import join

# Globals                              

SW_DIR = "/nfs/software/birney/"
DATA_DIR = "/nfs/research1/birney/projects/medaka/deepseqlab/"
(LANE_1, LANE_2, SAMPLES, PAIRS) = glob_wildcards(join(DATA_DIR, "H2YM2BBXY_MC259_19s000097-{lane_1}-1_Ibberson_lane{lane_2,\d+}{sample}_{pair}_sequence.txt.gz"))
REF_ZIP = "/nfs/research1/birney/users/brettell/ref/Oryzias_latipes.ASM223467v1.dna.toplevel.fa.gz"
REF_UNZIP = "/nfs/research1/birney/users/brettell/ref/Oryzias_latipes.ASM223467v1.dna.toplevel.fa"
CHR = ["1", "2", "3", "4", "5", "6", "7", "8", "9", "10", "11", "12", "13", "14", "15", "16", "17", "18", "19", "20", "21", "22", "23", "24", "MT"]
BWA = join(SW_DIR, "bwa/bwa")
PICARD = join(SW_DIR, "picard-2.9.0/picard.jar")
GATK = join(SW_DIR, "gatk-4.1.0.0/gatk")
BCF = join(SW_DIR, "bcftools/bcftools") # Version: 1.4-6-g5e49659 (using htslib 1.4-5-g10bc1a7)

# Rules                                

rule all:
    input:
        expand("gvcfs/gathered/{lane_1}_{sample}.g.vcf", lane_1 = LANE_1, sample = SAMPLES)

rule bwa_mem_1:
    input:
        l1_r1 = join(DATA_DIR, "H2YM2BBXY_MC259_19s000097-1-1_Ibberson_lane5{sample}_1_sequence.txt.gz"),
        l1_r2 = join(DATA_DIR, "H2YM2BBXY_MC259_19s000097-1-1_Ibberson_lane5{sample}_2_sequence.txt.gz")
    output:
        "mapped/1_{sample}.bam"
    shell:
        "{BWA} mem -M -t 16 {REF_ZIP} {input.l1_r1} {input.l1_r2} > {output}"

rule bwa_mem_2:
    input:
        l2_r1 = join(DATA_DIR, "H2YM2BBXY_MC259_19s000097-2-1_Ibberson_lane6{sample}_1_sequence.txt.gz"),
        l2_r2 = join(DATA_DIR, "H2YM2BBXY_MC259_19s000097-2-1_Ibberson_lane6{sample}_2_sequence.txt.gz")
    output:
        "mapped/2_{sample}.bam"
    shell:
        "{BWA} mem -M -t 16 {REF_ZIP} {input.l2_r1} {input.l2_r2} > {output}"

rule picard_add_read_groups:
    input:
        "mapped/{lane_1}_{sample}.bam"
    params:
        sample = "{sample}",
        lane = "{lane_1}"
    output:
        "added_read_groups/{lane_1}_{sample}.bam"
    shell:
        "java -Xmx50g -jar {PICARD} AddOrReplaceReadGroups I={input} O={output} RGID=1 RGLB=tailless RGPL=illumina RGPU={params.sample} RGSM={params.sample}_{params.lane} TMP_DIR=/nfs/research1/birney/users/brettell/tailess/added_read_groups"

rule picard_sort_sam:
    input:
        "added_read_groups/{lane_1}_{sample}.bam"
    output:
        bam = "sorted/{lane_1}_{sample}.bam",
        bai = "sorted/{lane_1}_{sample}.bai"
    shell:
        "java -Xmx50g -jar {PICARD} SortSam SO=coordinate INPUT={input} OUTPUT={output.bam} VALIDATION_STRINGENCY=LENIENT CREATE_INDEX=true TMP_DIR=/nfs/research1/birney/users/brettell/tailess/sorted"

rule picard_mark_duplicates:
    input:
        "sorted/{lane_1}_{sample}.bam"
    output:
        bam = "marked/{lane_1}_{sample}.bam",
        metrics = "marked/{lane_1}_{sample}.txt"
    shell:
        "java -Xmx50g -jar {PICARD} MarkDuplicates I={input} O={output.bam} M={output.metrics}"

rule picard_build_bam_index:
    input:
        "marked/{lane_1}_{sample}.bam"
    output:
        "marked/{lane_1}_{sample}.bai"
    shell:
        "java -Xmx50g -jar {PICARD} BuildBamIndex I={input}"

rule gatk_haplotype_caller:
    input:
        "marked/{lane_1}_{sample}.bam"
    params:
        chr = "{chr}"
    output:
        "gvcfs/{lane_1}_{sample}/{chr}.g.vcf.gz" # note: this can't use "{params.chr}" - needs to be {chr} from the global
    shell:
        "{GATK} --java-options \"-Xmx50G\" HaplotypeCaller -R {REF_UNZIP} -I {input} -L {params.chr} -O {output} -ERC GVCF"

rule bcftools_combine_vcfs:
    input:
        expand("gvcfs/{{lane_1}}_{{sample}}/{chr}.g.vcf.gz", chr=CHR)
    output:
        "gvcfs/gathered/{lane_1}_{sample}.g.vcf"
    shell:
        "{BCF} concat {input} -o {output}"
```

### Combine GVCFs using GenomicsDBImport

Tool docs here: <https://software.broadinstitute.org/gatk/documentation/tooldocs/current/org_broadinstitute_hellbender_tools_genomicsdb_GenomicsDBImport.php>.

#### Create map file
```{bash}
for i in $(ls gvcfs/gathered/ | cut -f1 -d'.'); do echo -e $i\\t$i.g.vcf; done > gvcfs/cohort.sample_map
```

#### Rerun combine for 2_KPK2
```{bash}
java -Xmx50g -jar /nfs/software/birney/picard-2.9.0/picard.jar GatherVcfs I=gvcfs/2_KPK2/1.g.vcf.gz I=gvcfs/2_KPK2/2.g
.vcf.gz I=gvcfs/2_KPK2/3.g.vcf.gz I=gvcfs/2_KPK2/4.g.vcf.gz I=gvcfs/2_KPK2/5.g.vcf.gz I=gvcfs/2_KPK2/6.g.vcf.gz I=gvcfs/2_KPK2/7.g.vcf.gz I=gvcfs/2_KPK2/8
.g.vcf.gz I=gvcfs/2_KPK2/9.g.vcf.gz I=gvcfs/2_KPK2/10.g.vcf.gz I=gvcfs/2_KPK2/11.g.vcf.gz I=gvcfs/2_KPK2/12.g.vcf.gz I=gvcfs/2_KPK2/13.g.vcf.gz I=gvcfs/2_
KPK2/14.g.vcf.gz I=gvcfs/2_KPK2/15.g.vcf.gz I=gvcfs/2_KPK2/16.g.vcf.gz I=gvcfs/2_KPK2/17.g.vcf.gz I=gvcfs/2_KPK2/18.g.vcf.gz I=gvcfs/2_KPK2/19.g.vcf.gz I=
gvcfs/2_KPK2/20.g.vcf.gz I=gvcfs/2_KPK2/21.g.vcf.gz I=gvcfs/2_KPK2/22.g.vcf.gz I=gvcfs/2_KPK2/23.g.vcf.gz I=gvcfs/2_KPK2/24.g.vcf.gz I=gvcfs/2_KPK2/MT.g.v
cf.gz O=gvcfs/gathered/2_KPK2.g.vcf.gz

/nfs/software/birney/gatk-4.1.0.0/gatk --java-options "-Xmx50G" IndexFeatureFile -F gvcfs/gathered/2_KPK2.g.vcf.gz
```


#### TRUE
```{python}
from os.path import join

# Globals                              

SW_DIR = "/nfs/software/birney/"
DATA_DIR = "/nfs/research1/birney/projects/medaka/deepseqlab/"
(LANE_1, LANE_2, SAMPLES, PAIRS) = glob_wildcards(join(DATA_DIR, "H2YM2BBXY_MC259_19s000097-{lane_1}-1_Ibberson_lane{lane_2,\d+}{sample}_{pair}_sequence.txt.gz"))
LANES = ["H2YM2BBXY_MC259_19s000097-1-1_Ibberson_lane5", "H2YM2BBXY_MC259_19s000097-2-1_Ibberson_lane6"]
LANES_2 = ["1_", "2_"]
LINES = ["KPK", "SOS", "STS"]
REPLS = ["1", "2"]
PAIRS = ["1", "2"]
REF_ZIP = "/nfs/research1/birney/users/brettell/ref/Oryzias_latipes.ASM223467v1.dna.toplevel.fa.gz"
REF_UNZIP = "/nfs/research1/birney/users/brettell/ref/Oryzias_latipes.ASM223467v1.dna.toplevel.fa"
CHR = ["1", "2", "3", "4", "5", "6", "7", "8", "9", "10", "11", "12", "13", "14", "15", "16", "17", "18", "19", "20", "21", "22", "23", "24", "MT"]
BWA = join(SW_DIR, "bwa/bwa")
PICARD = join(SW_DIR, "picard-2.9.0/picard.jar")
GATK = join(SW_DIR, "gatk-4.1.0.0/gatk")
BCF = join(SW_DIR, "bcftools/bcftools") # Version: 1.4-6-g5e49659 (using htslib 1.4-5-g10bc1a7)

# Rules                                

rule all:
    input:
        directory("genomics_db")

rule bwa_mem_1:
    input:
        l1_r1 = join(DATA_DIR, "H2YM2BBXY_MC259_19s000097-1-1_Ibberson_lane5{sample}_1_sequence.txt.gz"),
        l1_r2 = join(DATA_DIR, "H2YM2BBXY_MC259_19s000097-1-1_Ibberson_lane5{sample}_2_sequence.txt.gz")
    output:
        "mapped/1_{sample}.bam"
    shell:
        "{BWA} mem -M -t 16 {REF_ZIP} {input.l1_r1} {input.l1_r2} > {output}"

rule bwa_mem_2:
    input:
        l2_r1 = join(DATA_DIR, "H2YM2BBXY_MC259_19s000097-2-1_Ibberson_lane6{sample}_1_sequence.txt.gz"),
        l2_r2 = join(DATA_DIR, "H2YM2BBXY_MC259_19s000097-2-1_Ibberson_lane6{sample}_2_sequence.txt.gz")
    output:
        "mapped/2_{sample}.bam"
    shell:
        "{BWA} mem -M -t 16 {REF_ZIP} {input.l2_r1} {input.l2_r2} > {output}"

rule picard_add_read_groups:
    input:
        "mapped/{lane_1}_{sample}.bam"
    params:
        sample = "{sample}",
        lane = "{lane_1}"
    output:
        "added_read_groups/{lane_1}_{sample}.bam"
    shell:
        "java -Xmx50g -jar {PICARD} AddOrReplaceReadGroups I={input} O={output} RGID=1 RGLB=tailless RGPL=illumina RGPU={params.sample} RGSM={params.sample}_{params.lane} TMP_DIR=/nfs/research1/birney/users/brettell/tailess/added_read_groups"

rule picard_sort_sam:
    input:
        "added_read_groups/{lane_1}_{sample}.bam"
    output:
        bam = "sorted/{lane_1}_{sample}.bam",
        bai = "sorted/{lane_1}_{sample}.bai"
    shell:
        "java -Xmx50g -jar {PICARD} SortSam SO=coordinate INPUT={input} OUTPUT={output.bam} VALIDATION_STRINGENCY=LENIENT CREATE_INDEX=true TMP_DIR=/nfs/research1/birney/users/brettell/tailess/sorted"

rule picard_mark_duplicates:
    input:
        "sorted/{lane_1}_{sample}.bam"
    output:
        bam = "marked/{lane_1}_{sample}.bam",
        metrics = "marked/{lane_1}_{sample}.txt"
    shell:
        "java -Xmx50g -jar {PICARD} MarkDuplicates I={input} O={output.bam} M={output.metrics}"

rule picard_build_bam_index:
    input:
        "marked/{lane_1}_{sample}.bam"
    output:
        "marked/{lane_1}_{sample}.bai"
    shell:
        "java -Xmx50g -jar {PICARD} BuildBamIndex I={input}"

rule gatk_haplotype_caller:
    input:
        "marked/{lane_1}_{sample}.bam"
    params:
        chr = "{chr}"
    output:
        "gvcfs/{lane_1}_{sample}/{chr}.g.vcf.gz" # note: this can't use "{params.chr}" - needs to be {chr} from the global
    shell:
        "{GATK} --java-options \"-Xmx50G\" HaplotypeCaller -R {REF_UNZIP} -I {input} -L {params.chr} -O {output} -ERC GVCF"

rule picard_gather_vcfs:
    input:
        expand("gvcfs/{{lane_1}}_{{sample}}/{chr}.g.vcf.gz", chr=CHR)
    output:
        "gvcfs/gathered/{lane_1}_{sample}.g.vcf.gz"
    params:
        files = lambda wildcards, input: " I=".join(input)
    shell:
        "java -Xmx50g -jar {PICARD} GatherVcfs I={params.files} O={output}"

rule gatk_index_vcfs:
    input:
        "gvcfs/gathered/{lane_1}_{sample}.g.vcf.gz"
    output:
        "gvcfs/gathered/{lane_1}_{sample}.g.vcf.gz.tbi"
    shell:
        "{GATK} --java-options \"-Xmx50G\" IndexFeatureFile -F {input}"

rule gatk_genomics_db_import:
    input:
        expand("gvcfs/gathered/{lane_2}{line}{repl}.g.vcf.gz", lane_2 = LANES_2, line = LINES, repl = REPLS)
    output:
        directory("genomics_db")
    params:
        files = lambda wildcards, input: " -V ".join(input),
        intervals = lambda wildcards, input: " -L ".join(expand("{chr}", chr = CHR))
    shell:
        "{GATK} --java-options \"-Xmx40G -Xms40g\" GenomicsDBImport -V {params.files} --genomicsdb-workspace-path {output} --tmp-dir=tmp -L {params.intervals}"

# Submitted job 1 with external jobid 'Job <9754278> is submitted to queue <research-rh7>.'.   
# WORKS! But...
# Takes an extremely long time - 36 hours for the first 5 chromosomes. Try creating separate genomics databases for each chr then merging later.        
```

```{python}
from os.path import join

# Globals                              

SW_DIR = "/nfs/software/birney/"
DATA_DIR = "/nfs/research1/birney/projects/medaka/deepseqlab/"
(LANE_1, LANE_2, SAMPLES, PAIRS) = glob_wildcards(join(DATA_DIR, "H2YM2BBXY_MC259_19s000097-{lane_1}-1_Ibberson_lane{lane_2,\d+}{sample}_{pair}_sequence.txt.gz"))
LANES = ["H2YM2BBXY_MC259_19s000097-1-1_Ibberson_lane5", "H2YM2BBXY_MC259_19s000097-2-1_Ibberson_lane6"]
LANES_2 = ["1_", "2_"]
LINES = ["KPK", "SOS", "STS"]
REPLS = ["1", "2"]
PAIRS = ["1", "2"]
REF_ZIP = "/nfs/research1/birney/users/brettell/ref/Oryzias_latipes.ASM223467v1.dna.toplevel.fa.gz"
REF_UNZIP = "/nfs/research1/birney/users/brettell/ref/Oryzias_latipes.ASM223467v1.dna.toplevel.fa"
CHR = ["1", "2", "3", "4", "5", "6", "7", "8", "9", "10", "11", "12", "13", "14", "15", "16", "17", "18", "19", "20", "21", "22", "23", "24", "MT"]
BWA = join(SW_DIR, "bwa/bwa")
PICARD = join(SW_DIR, "picard-2.9.0/picard.jar")
GATK = join(SW_DIR, "gatk-4.1.0.0/gatk")
BCF = join(SW_DIR, "bcftools/bcftools") # Version: 1.4-6-g5e49659 (using htslib 1.4-5-g10bc1a7)

# Rules                                

rule all:
    input:
        expand(directory("genomics_db/{chr}"), chr = CHR)

rule bwa_mem_1:
    input:
        l1_r1 = join(DATA_DIR, "H2YM2BBXY_MC259_19s000097-1-1_Ibberson_lane5{sample}_1_sequence.txt.gz"),
        l1_r2 = join(DATA_DIR, "H2YM2BBXY_MC259_19s000097-1-1_Ibberson_lane5{sample}_2_sequence.txt.gz")
    output:
        "mapped/1_{sample}.bam"
    shell:
        "{BWA} mem -M -t 16 {REF_ZIP} {input.l1_r1} {input.l1_r2} > {output}"

rule bwa_mem_2:
    input:
        l2_r1 = join(DATA_DIR, "H2YM2BBXY_MC259_19s000097-2-1_Ibberson_lane6{sample}_1_sequence.txt.gz"),
        l2_r2 = join(DATA_DIR, "H2YM2BBXY_MC259_19s000097-2-1_Ibberson_lane6{sample}_2_sequence.txt.gz")
    output:
        "mapped/2_{sample}.bam"
    shell:
        "{BWA} mem -M -t 16 {REF_ZIP} {input.l2_r1} {input.l2_r2} > {output}"

rule picard_add_read_groups:
    input:
        "mapped/{lane_1}_{sample}.bam"
    params:
        sample = "{sample}",
        lane = "{lane_1}"
    output:
        "added_read_groups/{lane_1}_{sample}.bam"
    shell:
        "java -Xmx50g -jar {PICARD} AddOrReplaceReadGroups I={input} O={output} RGID=1 RGLB=tailless RGPL=illumina RGPU={params.sample} RGSM={params.sample}_{params.lane} TMP_DIR=/nfs/research1/birney/users/brettell/tailess/added_read_groups"

rule picard_sort_sam:
    input:
        "added_read_groups/{lane_1}_{sample}.bam"
    output:
        bam = "sorted/{lane_1}_{sample}.bam",
        bai = "sorted/{lane_1}_{sample}.bai"
    shell:
        "java -Xmx50g -jar {PICARD} SortSam SO=coordinate INPUT={input} OUTPUT={output.bam} VALIDATION_STRINGENCY=LENIENT CREATE_INDEX=true TMP_DIR=/nfs/research1/birney/users/brettell/tailess/sorted"

rule picard_mark_duplicates:
    input:
        "sorted/{lane_1}_{sample}.bam"
    output:
        bam = "marked/{lane_1}_{sample}.bam",
        metrics = "marked/{lane_1}_{sample}.txt"
    shell:
        "java -Xmx50g -jar {PICARD} MarkDuplicates I={input} O={output.bam} M={output.metrics}"

rule picard_build_bam_index:
    input:
        "marked/{lane_1}_{sample}.bam"
    output:
        "marked/{lane_1}_{sample}.bai"
    shell:
        "java -Xmx50g -jar {PICARD} BuildBamIndex I={input}"

rule gatk_haplotype_caller:
    input:
        "marked/{lane_1}_{sample}.bam"
    params:
        chr = "{chr}"
    output:
        "gvcfs/{lane_1}_{sample}/{chr}.g.vcf.gz" # note: this can't use "{params.chr}" - needs to be {chr} from the global
    shell:
        "{GATK} --java-options \"-Xmx50G\" HaplotypeCaller -R {REF_UNZIP} -I {input} -L {params.chr} -O {output} -ERC GVCF"

rule picard_gather_vcfs:
    input:
        expand("gvcfs/{{lane_1}}_{{sample}}/{chr}.g.vcf.gz", chr=CHR)
    output:
        "gvcfs/gathered/{lane_1}_{sample}.g.vcf.gz"
    params:
        files = lambda wildcards, input: " I=".join(input)
    shell:
        "java -Xmx50g -jar {PICARD} GatherVcfs I={params.files} O={output}"

rule gatk_index_vcfs:
    input:
        "gvcfs/gathered/{lane_1}_{sample}.g.vcf.gz"
    output:
        "gvcfs/gathered/{lane_1}_{sample}.g.vcf.gz.tbi"
    shell:
        "{GATK} --java-options \"-Xmx50G\" IndexFeatureFile -F {input}"

rule gatk_genomics_db_import:
    input:
        expand("gvcfs/gathered/{lane_2}{line}{repl}.g.vcf.gz", lane_2 = LANES_2, line = LINES, repl = REPLS)
    params:
        files = lambda wildcards, input: " -V ".join(input),
        chr = "{chr}"    
    output:
        directory("genomics_db/{chr}")
    shell:
        "{GATK} --java-options \"-Xmx40G -Xms40g\" GenomicsDBImport -V {params.files} --genomicsdb-workspace-path {output} --tmp-dir=tmp -L {params.chr} --reader-threads 5"

# SUCCESS        
```

### Genotype GVCFs

```{python}
from os.path import join

# Globals                              

SW_DIR = "/nfs/software/birney/"
DATA_DIR = "/nfs/research1/birney/projects/medaka/deepseqlab/"
(LANE_1, LANE_2, SAMPLES, PAIRS) = glob_wildcards(join(DATA_DIR, "H2YM2BBXY_MC259_19s000097-{lane_1}-1_Ibberson_lane{lane_2,\d+}{sample}_{pair}_sequence.txt.gz"))
LANES = ["H2YM2BBXY_MC259_19s000097-1-1_Ibberson_lane5", "H2YM2BBXY_MC259_19s000097-2-1_Ibberson_lane6"]
LANES_2 = ["1_", "2_"]
LINES = ["KPK", "SOS", "STS"]
REPLS = ["1", "2"]
PAIRS = ["1", "2"]
REF_ZIP = "/nfs/research1/birney/users/brettell/ref/Oryzias_latipes.ASM223467v1.dna.toplevel.fa.gz"
REF_UNZIP = "/nfs/research1/birney/users/brettell/ref/Oryzias_latipes.ASM223467v1.dna.toplevel.fa"
CHR = ["1", "2", "3", "4", "5", "6", "7", "8", "9", "10", "11", "12", "13", "14", "15", "16", "17", "18", "19", "20", "21", "22", "23", "24", "MT"]
BWA = join(SW_DIR, "bwa/bwa")
PICARD = join(SW_DIR, "picard-2.9.0/picard.jar")
GATK = join(SW_DIR, "gatk-4.1.0.0/gatk")
BCF = join(SW_DIR, "bcftools/bcftools") # Version: 1.4-6-g5e49659 (using htslib 1.4-5-g10bc1a7)

# Rules                                

rule all:
    input:
        expand("called/{chr}.vcf.gz", chr = CHR)

rule bwa_mem_1:
    input:
        l1_r1 = join(DATA_DIR, "H2YM2BBXY_MC259_19s000097-1-1_Ibberson_lane5{sample}_1_sequence.txt.gz"),
        l1_r2 = join(DATA_DIR, "H2YM2BBXY_MC259_19s000097-1-1_Ibberson_lane5{sample}_2_sequence.txt.gz")
    output:
        "mapped/1_{sample}.bam"
    shell:
        "{BWA} mem -M -t 16 {REF_ZIP} {input.l1_r1} {input.l1_r2} > {output}"

rule bwa_mem_2:
    input:
        l2_r1 = join(DATA_DIR, "H2YM2BBXY_MC259_19s000097-2-1_Ibberson_lane6{sample}_1_sequence.txt.gz"),
        l2_r2 = join(DATA_DIR, "H2YM2BBXY_MC259_19s000097-2-1_Ibberson_lane6{sample}_2_sequence.txt.gz")
    output:
        "mapped/2_{sample}.bam"
    shell:
        "{BWA} mem -M -t 16 {REF_ZIP} {input.l2_r1} {input.l2_r2} > {output}"

rule picard_add_read_groups:
    input:
        "mapped/{lane_1}_{sample}.bam"
    params:
        sample = "{sample}",
        lane = "{lane_1}"
    output:
        "added_read_groups/{lane_1}_{sample}.bam"
    shell:
        "java -Xmx50g -jar {PICARD} AddOrReplaceReadGroups I={input} O={output} RGID=1 RGLB=tailless RGPL=illumina RGPU={params.sample} RGSM={params.sample}_{params.lane} TMP_DIR=/nfs/research1/birney/users/brettell/tailess/added_read_groups"

rule picard_sort_sam:
    input:
        "added_read_groups/{lane_1}_{sample}.bam"
    output:
        bam = "sorted/{lane_1}_{sample}.bam",
        bai = "sorted/{lane_1}_{sample}.bai"
    shell:
        "java -Xmx50g -jar {PICARD} SortSam SO=coordinate INPUT={input} OUTPUT={output.bam} VALIDATION_STRINGENCY=LENIENT CREATE_INDEX=true TMP_DIR=/nfs/research1/birney/users/brettell/tailess/sorted"

rule picard_mark_duplicates:
    input:
        "sorted/{lane_1}_{sample}.bam"
    output:
        bam = "marked/{lane_1}_{sample}.bam",
        metrics = "marked/{lane_1}_{sample}.txt"
    shell:
        "java -Xmx50g -jar {PICARD} MarkDuplicates I={input} O={output.bam} M={output.metrics}"

rule picard_build_bam_index:
    input:
        "marked/{lane_1}_{sample}.bam"
    output:
        "marked/{lane_1}_{sample}.bai"
    shell:
        "java -Xmx50g -jar {PICARD} BuildBamIndex I={input}"

rule gatk_haplotype_caller:
    input:
        "marked/{lane_1}_{sample}.bam"
    params:
        chr = "{chr}"
    output:
        "gvcfs/{lane_1}_{sample}/{chr}.g.vcf.gz" # note: this can't use "{params.chr}" - needs to be {chr} from the global
    shell:
        "{GATK} --java-options \"-Xmx50G\" HaplotypeCaller -R {REF_UNZIP} -I {input} -L {params.chr} -O {output} -ERC GVCF"

rule picard_gather_vcfs:
    input:
        expand("gvcfs/{{lane_1}}_{{sample}}/{chr}.g.vcf.gz", chr=CHR)
    output:
        "gvcfs/gathered/{lane_1}_{sample}.g.vcf.gz"
    params:
        files = lambda wildcards, input: " I=".join(input)
    shell:
        "java -Xmx50g -jar {PICARD} GatherVcfs I={params.files} O={output}"

rule gatk_index_vcfs:
    input:
        "gvcfs/gathered/{lane_1}_{sample}.g.vcf.gz"
    output:
        "gvcfs/gathered/{lane_1}_{sample}.g.vcf.gz.tbi"
    shell:
        "{GATK} --java-options \"-Xmx50G\" IndexFeatureFile -F {input}"

rule gatk_genomics_db_import:
    input:
        expand("gvcfs/gathered/{lane_2}{line}{repl}.g.vcf.gz", lane_2 = LANES_2, line = LINES, repl = REPLS)
    params:
        files = lambda wildcards, input: " -V ".join(input),
        chr = "{chr}"    
    output:
        directory("genomics_db/{chr}")
    shell:
        "{GATK} --java-options \"-Xmx40G -Xms40g\" GenomicsDBImport -V {params.files} --genomicsdb-workspace-path {output} --tmp-dir=tmp -L {params.chr} --reader-threads 5"

rule gatk_genotype_gvcfs:
    input:
        directory("genomics_db/{chr}")
    output:
        "called/{chr}.vcf.gz"
    shell:
        "{GATK} --java-options \"-Xmx50G\" GenotypeGVCFs -R {REF_UNZIP} -V gendb://{input} -O {output} --tmp-dir=tmp"

## for chrMT
# ERROR: MissingOutputException in line 121 of /nfs/research1/birney/users/brettell/tailess/Snakefile:
#Missing files after 300 seconds:
#called/MT.vcf.gz
#This might be due to filesystem latency. If that is the case, consider to increase the wait time with --latency-wait.

## For chr13
#[TileDB::utils] Error: (gunzip) Cannot decompress with GZIP
#[TileDB::ReadState] Error: Cannot decompress tile.
#terminate called after throwing an instance of 'VariantStorageManagerException'
#  what():  VariantStorageManagerException exception : VariantArrayCellIterator increment failed
#TileDB error message : [TileDB::ReadState] Error: Cannot decompress tile

## for chr16
# ran for much longer than others (2 hours), and file didn't get updated afterwards.
```

Try running again for chr 13 and 16

```{bash}
bsub -M 50000 -Is bash
# 13
/nfs/software/birney/gatk-4.1.0.0/gatk --java-options "-Xmx50G" GenotypeGVCFs -R /nfs/research1/birney/users/brettell/ref/Oryzias_latipes.ASM223467v1.dna.toplevel.fa -V gendb://genomics_db/13 -O called/13.vcf.gz --tmp-dir=tmp
# SUCCESS

# 16
/nfs/software/birney/gatk-4.1.0.0/gatk --java-options "-Xmx50G" GenotypeGVCFs -R /nfs/research1/birney/users/brettell/ref/Oryzias_latipes.ASM223467v1.dna.toplevel.fa -V gendb://genomics_db/16 -O called/16.vcf.gz --tmp-dir=tmp
#SUCCESS

# MT
/nfs/software/birney/gatk-4.1.0.0/gatk --java-options "-Xmx50G" GenotypeGVCFs -R /nfs/research1/birney/users/brettell/ref/Oryzias_latipes.ASM223467v1.dna.toplevel.fa -V gendb://genomics_db/MT -O called/MT.vcf.gz --tmp-dir=tmp
#SUCCESS 
```

### Compile vcf
```{python}
from os.path import join

# Globals                              

SW_DIR = "/nfs/software/birney/"
DATA_DIR = "/nfs/research1/birney/projects/medaka/deepseqlab/"
(LANE_1, LANE_2, SAMPLES, PAIRS) = glob_wildcards(join(DATA_DIR, "H2YM2BBXY_MC259_19s000097-{lane_1}-1_Ibberson_lane{lane_2,\d+}{sample}_{pair}_sequence.txt.gz"))
LANES = ["H2YM2BBXY_MC259_19s000097-1-1_Ibberson_lane5", "H2YM2BBXY_MC259_19s000097-2-1_Ibberson_lane6"]
LANES_2 = ["1_", "2_"]
LINES = ["KPK", "SOS", "STS"]
REPLS = ["1", "2"]
PAIRS = ["1", "2"]
REF_ZIP = "/nfs/research1/birney/users/brettell/ref/Oryzias_latipes.ASM223467v1.dna.toplevel.fa.gz"
REF_UNZIP = "/nfs/research1/birney/users/brettell/ref/Oryzias_latipes.ASM223467v1.dna.toplevel.fa"
CHR = ["1", "2", "3", "4", "5", "6", "7", "8", "9", "10", "11", "12", "13", "14", "15", "16", "17", "18", "19", "20", "21", "22", "23", "24", "MT"]
BWA = join(SW_DIR, "bwa/bwa")
PICARD = join(SW_DIR, "picard-2.9.0/picard.jar")
GATK = join(SW_DIR, "gatk-4.1.0.0/gatk")
BCF = join(SW_DIR, "bcftools/bcftools") # Version: 1.4-6-g5e49659 (using htslib 1.4-5-g10bc1a7)

# Rules                                

rule all:
    input:
        "final/tailless.vcf.gz"

rule bwa_mem_1:
    input:
        l1_r1 = join(DATA_DIR, "H2YM2BBXY_MC259_19s000097-1-1_Ibberson_lane5{sample}_1_sequence.txt.gz"),
        l1_r2 = join(DATA_DIR, "H2YM2BBXY_MC259_19s000097-1-1_Ibberson_lane5{sample}_2_sequence.txt.gz")
    output:
        "mapped/1_{sample}.bam"
    shell:
        "{BWA} mem -M -t 16 {REF_ZIP} {input.l1_r1} {input.l1_r2} > {output}"

rule bwa_mem_2:
    input:
        l2_r1 = join(DATA_DIR, "H2YM2BBXY_MC259_19s000097-2-1_Ibberson_lane6{sample}_1_sequence.txt.gz"),
        l2_r2 = join(DATA_DIR, "H2YM2BBXY_MC259_19s000097-2-1_Ibberson_lane6{sample}_2_sequence.txt.gz")
    output:
        "mapped/2_{sample}.bam"
    shell:
        "{BWA} mem -M -t 16 {REF_ZIP} {input.l2_r1} {input.l2_r2} > {output}"

rule picard_add_read_groups:
    input:
        "mapped/{lane_1}_{sample}.bam"
    params:
        sample = "{sample}",
        lane = "{lane_1}"
    output:
        "added_read_groups/{lane_1}_{sample}.bam"
    shell:
        "java -Xmx50g -jar {PICARD} AddOrReplaceReadGroups I={input} O={output} RGID=1 RGLB=tailless RGPL=illumina RGPU={params.sample} RGSM={params.sample}_{params.lane} TMP_DIR=/nfs/research1/birney/users/brettell/tailess/added_read_groups"

rule picard_sort_sam:
    input:
        "added_read_groups/{lane_1}_{sample}.bam"
    output:
        bam = "sorted/{lane_1}_{sample}.bam",
        bai = "sorted/{lane_1}_{sample}.bai"
    shell:
        "java -Xmx50g -jar {PICARD} SortSam SO=coordinate INPUT={input} OUTPUT={output.bam} VALIDATION_STRINGENCY=LENIENT CREATE_INDEX=true TMP_DIR=/nfs/research1/birney/users/brettell/tailess/sorted"

rule picard_mark_duplicates:
    input:
        "sorted/{lane_1}_{sample}.bam"
    output:
        bam = "marked/{lane_1}_{sample}.bam",
        metrics = "marked/{lane_1}_{sample}.txt"
    shell:
        "java -Xmx50g -jar {PICARD} MarkDuplicates I={input} O={output.bam} M={output.metrics}"

rule picard_build_bam_index:
    input:
        "marked/{lane_1}_{sample}.bam"
    output:
        "marked/{lane_1}_{sample}.bai"
    shell:
        "java -Xmx50g -jar {PICARD} BuildBamIndex I={input}"

rule gatk_haplotype_caller:
    input:
        "marked/{lane_1}_{sample}.bam"
    params:
        chr = "{chr}"
    output:
        "gvcfs/{lane_1}_{sample}/{chr}.g.vcf.gz" # note: this can't use "{params.chr}" - needs to be {chr} from the global
    shell:
        "{GATK} --java-options \"-Xmx50G\" HaplotypeCaller -R {REF_UNZIP} -I {input} -L {params.chr} -O {output} -ERC GVCF"

rule picard_gather_vcfs:
    input:
        expand("gvcfs/{{lane_1}}_{{sample}}/{chr}.g.vcf.gz", chr=CHR)
    output:
        "gvcfs/gathered/{lane_1}_{sample}.g.vcf.gz"
    params:
        files = lambda wildcards, input: " I=".join(input)
    shell:
        "java -Xmx50g -jar {PICARD} GatherVcfs I={params.files} O={output}"

rule gatk_index_vcfs:
    input:
        "gvcfs/gathered/{lane_1}_{sample}.g.vcf.gz"
    output:
        "gvcfs/gathered/{lane_1}_{sample}.g.vcf.gz.tbi"
    shell:
        "{GATK} --java-options \"-Xmx50G\" IndexFeatureFile -F {input}"

rule gatk_genomics_db_import:
    input:
        expand("gvcfs/gathered/{lane_2}{line}{repl}.g.vcf.gz", lane_2 = LANES_2, line = LINES, repl = REPLS)
    params:
        files = lambda wildcards, input: " -V ".join(input),
        chr = "{chr}"    
    output:
        directory("genomics_db/{chr}")
    shell:
        "{GATK} --java-options \"-Xmx40G -Xms40g\" GenomicsDBImport -V {params.files} --genomicsdb-workspace-path {output} --tmp-dir=tmp -L {params.chr} --reader-threads 5"

rule gatk_genotype_gvcfs:
    input:
        directory("genomics_db/{chr}")
    output:
        "called/{chr}.vcf.gz"
    shell:
        "{GATK} --java-options \"-Xmx50G\" GenotypeGVCFs -R {REF_UNZIP} -V gendb://{input} -O {output} --tmp-dir=tmp"

rule picard_gather_final:
    input:
        expand("called/{chr}.vcf.gz", chr=CHR)
    output:
        "final/tailless.vcf.gz"
    params:
        files = lambda wildcards, input: " I=".join(input)    
    shell:
        "java -Xmx50g -jar {PICARD} GatherVcfs I={params.files} O={output}"
```

*18 March 2019*

Nature article on mice with heads clamped while "travelling" through a VR tunnel: <https://www.nature.com/articles/d41586-019-00791-w?utm_source=Nature+Briefing&utm_campaign=934c9a01b5-briefing-dy-20190312&utm_medium=email&utm_term=0_c9dfd39373-934c9a01b5-43358109>. 

References fish with their head trapped in gel, monitoring 