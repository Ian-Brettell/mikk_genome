Building DAG of jobs...
Using shell: /usr/local/bin/bash
Provided cluster nodes: 5000
Job counts:
	count	jobs
	1	all
	1	mark_kaga_duplicates
	2
Select jobs to execute...

[Tue Mar 23 17:01:41 2021]
rule mark_kaga_duplicates:
    input: ../introgression/sams/kaga.sam
    output: ../introgression/bams/kaga.bam
    jobid: 9616


        gatk MarkDuplicatesSpark             -I ../introgression/sams/kaga.sam             -O ../introgression/bams/kaga.bam
        
Submitted job 9616 with external jobid 'Job <4223403> is submitted to default queue <research-rh74>.'.
[Tue Mar 23 17:03:51 2021]
Error in rule mark_kaga_duplicates:
    jobid: 9616
    output: ../introgression/bams/kaga.bam
    shell:
        
        gatk MarkDuplicatesSpark             -I ../introgression/sams/kaga.sam             -O ../introgression/bams/kaga.bam
        
        (one of the commands exited with non-zero exit code; note that snakemake uses bash strict mode!)
    cluster_jobid: Job <4223403> is submitted to default queue <research-rh74>.

Error executing rule mark_kaga_duplicates on cluster (jobid: 9616, external: Job <4223403> is submitted to default queue <research-rh74>., jobscript: /hps/research1/birney/users/ian/mikk_paper/mikk_genome/.snakemake/tmp.676ubu9g/snakejob.mark_kaga_duplicates.9616.sh). For error details see the cluster log and the log files of the involved rule(s).
Job failed, going on with independent jobs.
Exiting because a job execution failed. Look above for error message
Complete log: /hps/research1/birney/users/ian/mikk_paper/mikk_genome/.snakemake/log/2021-03-23T165953.068267.snakemake.log
